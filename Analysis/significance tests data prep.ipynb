{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('C:/Users/Mike/Desktop/word_embeddings-master') # windows dir\n",
    "os.chdir('/home/mike/Desktop/Word Embeddings') # Linux dir\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from functools import reduce\n",
    "\n",
    "os.chdir('./Text Processing')\n",
    "from TextPrep import TextPrep\n",
    "\n",
    "os.chdir('../Meta Data')\n",
    "from key_words import key_words, key_synonyms, base_words, base_synonyms, agree_words, agree_synonyms\n",
    "from stop_words import stop_words\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for analyzing the word vectors\n",
    "# returns the cosine similarity of two words\n",
    "def cosine_sim(parser, keyword, text, labels, relative = False):\n",
    "    # define tagged keywords. To generalize get a list of unique labels. loop through create a new variable for each label\n",
    "    keyword_r = keyword + '_r'\n",
    "    keyword_d = keyword + '_d'\n",
    "    \n",
    "    ptweets = []\n",
    "    for i in range(len(text)):\n",
    "        try:\n",
    "            ptweets.append(parser.tag_keywords(keyword, text[i], labels[i])) # tweets and labels are global variables. change to local\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('failed at '+ keyword + str(i))\n",
    "            \n",
    "    # lemmatize\n",
    "    ptweets = parser.multi_lemmatizer(ptweets, threads = 6)\n",
    "\n",
    "    # drop single letters\n",
    "    for i in range(len(ptweets)):\n",
    "        ptweets[i] = [word for word in ptweets[i] if len(word) > 1]\n",
    "\n",
    "    # train and save word2vec\n",
    "    pmodel = Word2Vec(ptweets, window = 10, sg = 1)\n",
    "    \n",
    "    # return cosine similarity between the words\n",
    "    if relative == True:\n",
    "        return pmodel.wv.relative_cosine_similarity(keyword_r, keyword_d, topn=10)\n",
    "    else:\n",
    "        return pmodel.wv.similarity(keyword_r, keyword_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "meta_data = pd.read_csv('Meta Data/meta_data.csv')\n",
    "tweet_df = pd.read_csv('Data/aggregated_tweets.csv')\n",
    "# subset to tweets after Nov. 6\n",
    "tweet_df = tweet_df[(tweet_df['created'] >= '2019-11-06') & \n",
    "                   (tweet_df['created'] <= '2019-12-16')]\n",
    "# merge data with meta data\n",
    "tweet_df = pd.merge(tweet_df, meta_data, how = 'inner', on = 'user_id')\n",
    "tweet_df = tweet_df[tweet_df.party.isin(['R', 'D'])].reset_index(drop=True)\n",
    "\n",
    "tweets = tweet_df['text']\n",
    "labels = tweet_df['party']\n",
    "\n",
    "# initialize parser for both keywords and base words\n",
    "keyprep = TextPrep(stopwords = stop_words, key_words = key_words, key_synonyms = key_synonyms)\n",
    "baseprep = TextPrep(stopwords = stop_words, key_words = base_words, key_synonyms = base_synonyms)\n",
    "agreeprep = TextPrep(stopwords = stop_words, key_words = agree_words, key_synonyms = agree_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 734 ms, sys: 4.1 ms, total: 738 ms\n",
      "Wall time: 736 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preprocess text\n",
    "tweets = [keyprep.twitter_preprocess(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get cosine sim for key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37min 5s, sys: 11.6 s, total: 37min 16s\n",
      "Wall time: 27min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get cosine similarity for all words in the key word list\n",
    "keysim = []\n",
    "for word in key_words:\n",
    "    try:\n",
    "        cosine = cosine_sim(parser = keyprep, keyword = word, text = tweets, labels = labels)\n",
    "        keysim.append(cosine)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('failed at ' + word)\n",
    "\n",
    "# Convert to dataframe\n",
    "keysimdf = pd.DataFrame(data=list(zip(key_words, keysim)), columns = ['word', 'similarity'])\n",
    "keysimdf.to_csv('Analysis/keyword_similarity.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get cosine sim for base words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56min 48s, sys: 18.2 s, total: 57min 7s\n",
      "Wall time: 41min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get cosine similarity for all words in the base word list\n",
    "basesim = []\n",
    "for word in base_words:\n",
    "    try:\n",
    "        cosine = cosine_sim(parser = baseprep, keyword = word, text = tweets, labels = labels)\n",
    "        basesim.append(cosine)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('failed at ' + word)\n",
    "# Convert to dataframe\n",
    "basesimdf = pd.DataFrame(data=list(zip(base_words, basesim)), columns = ['word', 'similarity'])\n",
    "basesimdf.to_csv('Analysis/baseword_similarity.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get cosine sim for agree words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 36s, sys: 3.75 s, total: 11min 40s\n",
      "Wall time: 8min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get cosine similarity for all words in the base word list\n",
    "agreesim = []\n",
    "for word in agree_words:\n",
    "    try:\n",
    "        cosine = cosine_sim(parser = agreeprep, keyword = word, text = tweets, labels = labels)\n",
    "        agreesim.append(cosine)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('failed at ' + word)\n",
    "# Convert to dataframe\n",
    "agreesimdf = pd.DataFrame(data=list(zip(agree_words, agreesim)), columns = ['word', 'similarity'])\n",
    "agreesimdf.to_csv('Analysis/agreeword_similarity.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.570209825861043"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(keysim)/len(keysim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6012013243867996"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(basesim)/len(basesim)\n",
    "#basesim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7672532081604004"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(agreesim)/len(agreesim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Cosine similarity tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34min 10s, sys: 31.4 s, total: 34min 41s\n",
      "Wall time: 25min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get cosine similarity for all words in the key word list\n",
    "keyrelsim = []\n",
    "for word in key_words:\n",
    "    try:\n",
    "        cosine = cosine_sim(parser = keyprep, keyword = word, text = tweets, labels = labels, relative = True)\n",
    "        keyrelsim.append(cosine)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('failed at ' + word)\n",
    "\n",
    "# Convert to dataframe\n",
    "keyrelsimdf = pd.DataFrame(data=list(zip(key_words, keyrelsim)), columns = ['word', 'relative_similarity'])\n",
    "keyrelsimdf.to_csv('Data/keyword_relative_similarity.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07482782923600337"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(keyrelsim)/len(keyrelsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58min 44s, sys: 52.2 s, total: 59min 36s\n",
      "Wall time: 43min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get relative cosine similarity for all words in the base word list\n",
    "baserelsim = []\n",
    "for word in base_words:\n",
    "    try:\n",
    "        cosine = cosine_sim(parser = baseprep, keyword = word, text = tweets, labels = labels, relative = True)\n",
    "        baserelsim.append(cosine)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('failed at ' + word)\n",
    "# Convert to dataframe\n",
    "baserelsimdf = pd.DataFrame(data=list(zip(base_words, baserelsim)), columns = ['word', 'relative_similarity'])\n",
    "baserelsimdf.to_csv('Data/baseword_relative_similarity.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09539504181767948"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(baserelsim)/len(baserelsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agree words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 44s, sys: 10.8 s, total: 12min 55s\n",
      "Wall time: 9min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "agreerelsim = []\n",
    "for word in agree_words:\n",
    "    try:\n",
    "        cosine = cosine_sim(parser = agreeprep, keyword = word, text = tweets, labels = labels, relative = True)\n",
    "        agreerelsim.append(cosine)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('failed at ' + word)\n",
    "# Convert to dataframe\n",
    "agreerelsimdf = pd.DataFrame(data=list(zip(agree_words, agreerelsim)), columns = ['word', 'relative_similarity'])\n",
    "agreerelsimdf.to_csv('Data/agreeword_relative_similarity.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09952264188657398"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(agreerelsim)/len(agreerelsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine to a single DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels to the words\n",
    "keysimdf['label'] = 'disagree'\n",
    "basesimdf['label'] = 'base'\n",
    "agreesimdf['label'] = 'agree'\n",
    "\n",
    "keyrelsimdf['label'] = 'disagree'\n",
    "baserelsimdf['label'] = 'base'\n",
    "agreerelsimdf['label'] = 'agree'\n",
    "\n",
    "# concat cosine and relative cosine into two frames\n",
    "cosine_frames = pd.concat([keysimdf, basesimdf, agreesimdf])\n",
    "relative_frames = pd.concat([keyrelsimdf, baserelsimdf, agreerelsimdf])\n",
    "# merge ingo a single df\n",
    "df_merged = pd.merge(cosine_frames, relative_frames, how='left', on = ['word', 'label'])\n",
    "\n",
    "df_merged.to_csv(\"word_similarities.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to democrat and republican tweets\n",
    "countdf = pd.DataFrame(data = list(zip(tweets,labels)), columns = ['text', 'party'])\n",
    "dems = countdf[countdf['party'] == 'D']\n",
    "demtext = dems['text']\n",
    "reps = countdf[countdf['party'] == 'R']\n",
    "reptext = reps['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abortion: 44\n",
      "administration: 568\n",
      "border: 110\n",
      "conservative: 12\n",
      "corrupt: 273\n",
      "climatechange: 293\n",
      "democrat: 806\n",
      "daca: 289\n",
      "economy: 327\n",
      "gun: 614\n",
      "healthcare: 688\n",
      "impeach: 983\n",
      "immigration: 99\n",
      "insurance: 210\n",
      "liberal: 60\n",
      "mcconnell: 465\n",
      "oil: 103\n",
      "president: 1740\n",
      "pelosi: 142\n",
      "police: 146\n",
      "republican: 851\n",
      "russia: 173\n",
      "scotus: 295\n",
      "tax: 316\n",
      "trump: 2350\n",
      "usmca: 29\n",
      "wall: 98\n",
      "wealth: 83\n",
      "welfare: 21\n",
      "whitehouse: 236\n"
     ]
    }
   ],
   "source": [
    "# count instances of each word used by democrats\n",
    "for word in key_words_small:\n",
    "    count_text = []\n",
    "    for text in list(demtext):\n",
    "        if word in keyprep.key_synonyms.values():\n",
    "            count_text.append(keyprep.replace_synonyms(word, text))\n",
    "        else:\n",
    "            count_text.append(text)\n",
    "    dcount = str(len([text for text in count_text if re.search(word, text.lower())]))\n",
    "    print(word + ':' + ' ' + dcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abortion: 22\n",
      "administration: 116\n",
      "border: 133\n",
      "conservative: 25\n",
      "corrupt: 101\n",
      "climatechange: 7\n",
      "democrat: 1363\n",
      "daca: 6\n",
      "economy: 319\n",
      "gun: 42\n",
      "healthcare: 143\n",
      "impeach: 1706\n",
      "immigration: 33\n",
      "insurance: 25\n",
      "liberal: 62\n",
      "mcconnell: 17\n",
      "oil: 36\n",
      "president: 878\n",
      "pelosi: 385\n",
      "police: 116\n",
      "republican: 355\n",
      "russia: 117\n",
      "scotus: 22\n",
      "tax: 231\n",
      "trump: 1501\n",
      "usmca: 513\n",
      "wall: 70\n",
      "wealth: 16\n",
      "welfare: 6\n",
      "whitehouse: 92\n"
     ]
    }
   ],
   "source": [
    "# count instances of each word used by republicans\n",
    "for word in key_words_small:\n",
    "    count_text = []\n",
    "    for text in list(reptext):\n",
    "        if word in keyprep.key_synonyms.values():\n",
    "            count_text.append(keyprep.replace_synonyms(word, text))\n",
    "        else:\n",
    "            count_text.append(text)\n",
    "    rcount = str(len([text for text in count_text if re.search(word, text.lower())]))\n",
    "    print(word + ':' + ' ' + rcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer: 43\n",
      "education: 77\n",
      "infrastructure: 105\n",
      "isis: 115\n",
      "kurd: 6\n",
      "service: 646\n",
      "terrorism: 80\n",
      "veteran: 855\n"
     ]
    }
   ],
   "source": [
    "# count instances of each agree word used by republicans\n",
    "for word in agree_words:\n",
    "    count_text = []\n",
    "    for text in list(reptext):\n",
    "        if word in agreeprep.key_synonyms.values():\n",
    "            count_text.append(agreeprep.replace_synonyms(word, text))\n",
    "        else:\n",
    "            count_text.append(text)\n",
    "    rcount = str(len([text for text in count_text if re.search(word, text.lower())]))\n",
    "    print(word + ':' + ' ' + rcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer: 87\n",
      "education: 304\n",
      "infrastructure: 124\n",
      "isis: 451\n",
      "kurd: 32\n",
      "service: 1248\n",
      "terrorism: 61\n",
      "veteran: 1284\n"
     ]
    }
   ],
   "source": [
    "for word in agree_words:\n",
    "    count_text = []\n",
    "    for text in list(demtext):\n",
    "        if word in agreeprep.key_synonyms.values():\n",
    "            count_text.append(agreeprep.replace_synonyms(word, text))\n",
    "        else:\n",
    "            count_text.append(text)\n",
    "    rcount = str(len([text for text in count_text if re.search(word, text.lower())]))\n",
    "    print(word + ':' + ' ' + rcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: 148, 250\n",
      "annual: 54, 125\n",
      "able: 617, 1694\n",
      "bring: 185, 313\n",
      "cancer: 33, 71\n",
      "come: 374, 740\n",
      "chance: 48, 119\n",
      " day : 318, 447\n",
      "entire: 82, 98\n",
      "far: 346, 296\n",
      "find: 99, 310\n",
      "go: 1374, 2398\n",
      "get: 801, 1421\n",
      "hear: 909, 1372\n",
      "help: 697, 1348\n",
      "host: 138, 281\n",
      "hold: 246, 606\n",
      " join : 244, 464\n",
      "look: 275, 374\n",
      "long: 315, 577\n",
      "like: 414, 778\n",
      "live: 551, 1096\n",
      "month: 181, 321\n",
      "matter: 65, 166\n",
      "member: 503, 863\n",
      "morning: 261, 353\n",
      "meet: 285, 490\n",
      "night: 212, 318\n",
      "near: 108, 242\n",
      "opportunity: 172, 320\n",
      "open: 172, 406\n",
      "plan: 147, 545\n",
      "place: 134, 437\n",
      "phone: 145, 135\n",
      "read: 281, 464\n",
      "receive: 156, 199\n",
      "recent: 133, 150\n",
      "sure: 556, 1349\n",
      "send: 81, 147\n",
      "share: 110, 245\n",
      "small: 175, 271\n",
      "staff: 129, 224\n",
      "shut: 39, 54\n",
      "thanksgiving: 9, 6\n",
      " thank : 230, 289\n",
      "think: 129, 198\n",
      "take: 357, 833\n",
      "today: 944, 1530\n",
      "talk: 273, 303\n",
      "weekend: 81, 139\n",
      " week : 235, 273\n",
      "yesterday: 158, 174\n"
     ]
    }
   ],
   "source": [
    "for word in base_words:\n",
    "    rcount = str(len(list(filter(lambda x: word in x, reptext))))\n",
    "    dcount = str(len(list(filter(lambda x: word in x, demtext))))\n",
    "    print(word + ':' + ' ' + rcount +', ' + dcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Despite all the challenges facing our nation, Democrats have focused on nothing but getting rid of @realDonaldTrump since the day he was elected. \\r\\n\\r\\nâŒThey failed with collusion. \\r\\nâŒThey failed with the Mueller Report. \\r\\nâŒAnd they're going to fail again with their #ShamImpeachment https://t.co/RC2x7EMLfr\",\n",
       " \"Devastating headline for Democrats:\\r\\n\\r\\nðŸš¨ â€˜No One Believes Anythingâ€™: Voters Worn Out by a Fog of Political News ðŸš¨\\r\\n\\r\\nFrom collusion to obstruction of justice to abuse of power to quid pro quo to bribery/extortion, Democrats have cried 'wolf' too many times. https://t.co/aD7SSdbMdD\",\n",
       " 'The past three years have been marked by false and misleading stories and narratives to perpetuate the Democratsâ€™ Russian collusion hoax and Ukrainian phone call hoax.\\r\\n\\r\\nItâ€™s time to end this sham against @realDonaldTrump and those who support him. https://t.co/6lh5wPA18s',\n",
       " 'ðŸš¨ More collusion between Dems &amp; their star witnesses revealed:\\r\\n\\r\\nYovanovitch communicated with Democrat staff BEFORE the whistleblower report was even public &amp; then lied under oath trying to cover up the secret dealings.\\r\\n\\r\\nJust further proof this is rigged! https://t.co/DuPS75cvsF',\n",
       " 'Schiff said weâ€™d hear from the whistleblower.\\r\\n\\r\\nBut when collusion between the whistleblower &amp; Schiffâ€™s staff was revealed, he changed his tune.\\r\\n\\r\\nRepublicans requested the whistleblower as a witness in Schiffâ€™s show hearings. Will he block him out to avoid people seeing the truth?',\n",
       " 'The republic can survive the truth about the Russia collusion folly, writes Holman W. Jenkins Jr. https://t.co/BualJvAoZy via @WSJ',\n",
       " '2019 is almost over, and we have managed to waste nearly all of it on a sham impeachment process. First it was collusion, then obstruction, and now quid pro quo. What will they come up with next?',\n",
       " 'No collusion. No obstruction. No quid pro quo.\\r\\n\\r\\nHow much longer will Democrats insult the American peopleâ€™s intelligence with this sham impeachment exercise?',\n",
       " \"We finally have some clarity on when we'll get the long-awaited Justice Department IG report on #FISA abuse.\\r\\n\\r\\nHorowitz's report has the potential to blow the lid off the origins of the Russia collusion hoax and those in the #DeepState who conspired against @realDonaldTrump. https://t.co/M8JJWQMWv1\",\n",
       " 'After believing @RepAdamSchiff for 2+ years on phony Russia collusion, Democrats had the poor judgment to believe him again. They walked over the impeachment cliff as a whole party.\\r\\n\\r\\nNow theyâ€™re busted and America is calling their hand.\\r\\n\\r\\nSchiff should be investigated &amp; removed. https://t.co/ZHz46ZoEfw',\n",
       " \".@HouseDemocrats have thrown every accusation at @POTUS @realdonaldtrump they can conjure:\\r\\n \\r\\nâ€” Russian collusion\\r\\nâ€” Obstruction of justice\\r\\nâ€” Quid pro quo \\r\\nâ€” Bribery\\r\\nâ€” Witness intimidation\\r\\nâ€” High crimes and misdemeanors\\r\\nâ€” Violating the emolumentâ€™s clause\\r\\n\\r\\nAll false...what's next?\",\n",
       " 'This Ukraine fairy tale is even more of a dud than the Russia collusion hoax, but desperate Dems call for desperate measures.\\r\\n\\r\\nAdam Schiff definitely does NOT want you to read &amp; RT this:\\r\\n\\r\\nhttps://t.co/29tEpHfQsu',\n",
       " 'In a desperate attempt to find an impeachment charge that will stick, Speaker Pelosi has moved the goalposts again: Russian collusion, obstruction of justice, quid pro quo, and now bribery? What will Democrats try next? https://t.co/AwERHEeL4M',\n",
       " 'If Democrats are accusing @realDonaldTrump of bribery b/c they think a new lie will float better in battlegrounds, they forget Americans already saw through Schiffâ€™s lies about collusion &amp; the July 25 call. \\r\\n\\r\\nThe false narratives are worn out. \\r\\n\\r\\nAmericansâ€™ patience is worn thin.',\n",
       " 'This will go well with the evidence of â€œRussian collusionâ€ we also heard about https://t.co/V9BPp7CCpw',\n",
       " 'Setting aside the ridiculous spin here, tell me: who was Chairman Schiff speaking to when he spread Russian collusion conspiracy theories for 2 years? https://t.co/ARDeiUuTo0',\n",
       " 'Adam Schiffâ€™s impeachment screen play isnâ€™t turning out as he hoped. Act I was the Russia collusion hoax, which failed to give him his chosen ending. Then came Act II, the tall tale of Ukraine. \\r\\n\\r\\nThis sham inquiry is DC politics at its worst and Americans know it.',\n",
       " 'Lying to the American people has become a pattern for Adam Schiff. He claimed to have evidence of Russian collusion, read a fake transcript of President Trump\\'s phone call, and was caught red handed coordinating with the \"whistleblower.\"',\n",
       " 'Maxine Waters has called for impeachment since 2017. It was never about \"collusion\" or Ukraine; Democrats were always going to try to impeach President Trump.\\r\\nhttps://t.co/YJTIPRfTM2',\n",
       " 'Even The New York Times claims the Democrats are using this Ukraine hoax as a â€œdo-overâ€ for their failed Russian collusion narrative to bring down President @realDonaldTrump. https://t.co/6BZpWwU41e',\n",
       " 'Ah I see. Democrat witnesses are re-litigating Special Counsel report that found no collusion, no obstruction, after they put the country through 2 years of Hell because Donald Trump won an election. Still living in fantasy world of Collusion Delusion. Deja vu all over again!',\n",
       " \"I think there's a story there. How did this false narrative of Russian collusion with the @realdonaldtrump campaign even begin? https://t.co/za2hXFHl23\",\n",
       " 'Well itâ€™s clear now, Schiff wanted to hide the fact that he clearly didnâ€™t have the goods he was selling. Kind of like his Russia collusion claims. We need to end this #ShamImpeachment now. https://t.co/PTQO7PLEwK']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'administration' in prep.key_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im live this morning on kfor with laceylett great talk on reducing prescription drug costs and my upcoming community conversations in oklahoma\n"
     ]
    }
   ],
   "source": [
    "if 'administration' in prep.key_synonyms:\n",
    "    print(prep.replace_synonyms('administration', tweets[0]))\n",
    "print(tweets[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
