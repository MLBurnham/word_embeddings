{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anaconda-client==1.7.2\n",
      "anaconda-navigator==1.9.7\n",
      "asn1crypto==1.2.0\n",
      "astroid==2.2.5\n",
      "atomicwrites==1.3.0\n",
      "attrs==19.3.0\n",
      "autopep8==1.4.4\n",
      "backcall==0.1.0\n",
      "backports.functools-lru-cache==1.5\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "beautifulsoup4==4.8.1\n",
      "bleach==3.1.0\n",
      "blis==0.2.4\n",
      "boto==2.49.0\n",
      "boto3==1.9.222\n",
      "botocore==1.12.222\n",
      "certifi==2019.9.11\n",
      "cffi==1.13.1\n",
      "chardet==3.0.4\n",
      "Click==7.0\n",
      "clyent==1.2.2\n",
      "conda==4.7.12\n",
      "conda-build==3.18.8\n",
      "conda-package-handling==1.6.0\n",
      "conda-verify==3.4.2\n",
      "cryptography==2.8\n",
      "cycler==0.10.0\n",
      "cymem==2.0.2\n",
      "decorator==4.4.1\n",
      "defusedxml==0.6.0\n",
      "demoji==0.1.5\n",
      "docutils==0.15.2\n",
      "en-core-web-sm==2.1.0\n",
      "entrypoints==0.3\n",
      "filelock==3.0.12\n",
      "funcy==1.13\n",
      "future==0.17.1\n",
      "fuzzyset==0.0.19\n",
      "gensim==3.8.0\n",
      "glob2==0.7\n",
      "idna==2.8\n",
      "importlib-metadata==0.23\n",
      "ipykernel==5.1.3\n",
      "ipython==7.9.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.5.1\n",
      "isort==4.3.21\n",
      "jedi==0.14.1\n",
      "Jinja2==2.10.3\n",
      "jmespath==0.9.4\n",
      "joblib==0.13.2\n",
      "json5==0.8.5\n",
      "jsonschema==3.1.1\n",
      "jupyter-client==5.3.4\n",
      "jupyter-core==4.6.0\n",
      "jupyterlab==1.1.4\n",
      "jupyterlab-server==1.0.6\n",
      "kiwisolver==1.1.0\n",
      "lazy-object-proxy==1.4.2\n",
      "lief==0.9.0\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.1.1\n",
      "mccabe==0.6.1\n",
      "mistune==0.8.4\n",
      "mkl-fft==1.0.14\n",
      "mkl-random==1.1.0\n",
      "mkl-service==2.3.0\n",
      "more-itertools==7.2.0\n",
      "murmurhash==1.0.2\n",
      "navigator-updater==0.2.1\n",
      "nbconvert==5.6.0\n",
      "nbformat==4.4.0\n",
      "nltk==3.4.5\n",
      "notebook==6.0.1\n",
      "numexpr==2.7.0\n",
      "numpy==1.17.2\n",
      "oauthlib==3.1.0\n",
      "olefile==0.46\n",
      "packaging==19.1\n",
      "pandas==0.25.2\n",
      "pandocfilters==1.4.2\n",
      "parso==0.5.1\n",
      "pexpect==4.7.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==6.2.0\n",
      "pkginfo==1.5.0.1\n",
      "plac==0.9.6\n",
      "pluggy==0.12.0\n",
      "preshed==2.0.1\n",
      "prometheus-client==0.7.1\n",
      "prompt-toolkit==2.0.10\n",
      "psutil==5.6.3\n",
      "ptyprocess==0.6.0\n",
      "py==1.8.0\n",
      "pycodestyle==2.5.0\n",
      "pycosat==0.6.3\n",
      "pycparser==2.19\n",
      "pydocstyle==4.0.1\n",
      "pyflakes==2.1.1\n",
      "Pygments==2.4.2\n",
      "pyLDAvis==2.1.2\n",
      "pylint==2.3.1\n",
      "pyOpenSSL==19.0.0\n",
      "pyparsing==2.4.2\n",
      "PyPDF2==1.26.0\n",
      "pyrsistent==0.15.4\n",
      "PySocks==1.7.1\n",
      "pytest==5.1.2\n",
      "python-dateutil==2.8.0\n",
      "python-jsonrpc-server==0.2.0\n",
      "python-language-server==0.28.2\n",
      "python-Levenshtein==0.12.0\n",
      "pytz==2019.3\n",
      "PyYAML==5.1.2\n",
      "pyzmq==18.1.0\n",
      "QtPy==1.9.0\n",
      "requests==2.22.0\n",
      "requests-oauthlib==1.2.0\n",
      "rope==0.14.0\n",
      "ruamel-yaml==0.15.46\n",
      "s3transfer==0.2.1\n",
      "scikit-learn==0.21.3\n",
      "scipy==1.3.1\n",
      "Send2Trash==1.5.0\n",
      "six==1.12.0\n",
      "smart-open==1.8.4\n",
      "snowballstemmer==1.9.0\n",
      "soupsieve==1.9.3\n",
      "spacy==2.1.8\n",
      "srsly==0.1.0\n",
      "terminado==0.8.2\n",
      "testpath==0.4.2\n",
      "texttable==1.6.2\n",
      "thinc==7.0.8\n",
      "tornado==6.0.3\n",
      "tqdm==4.36.1\n",
      "traitlets==4.3.3\n",
      "tweepy==3.8.0\n",
      "typed-ast==1.4.0\n",
      "urllib3==1.24.2\n",
      "wasabi==0.2.2\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "widgetsnbextension==3.5.1\n",
      "wrapt==1.11.2\n",
      "yapf==0.28.0\n",
      "zipp==0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/mike/Desktop/Word Embeddings')\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "os.chdir('./Text Processing')\n",
    "from TextPrep import TextPrep\n",
    "\n",
    "os.chdir('../Meta Data')\n",
    "from key_words import key_words_small, key_synonyms, base_words, base_synonyms\n",
    "from stop_words import stop_words\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for analyzing the word vectors\n",
    "\n",
    "# gets most similar words\n",
    "def similar_words(word, model, topn):\n",
    "    sim_words = []\n",
    "    for i in range(topn):\n",
    "        sim_words.append(model.wv.most_similar(word, topn = topn)[i][0])\n",
    "    return sim_words + [word]\n",
    "\n",
    "# returns a dictionary for the similar words. values are the key words, keys are the most similar words\n",
    "def similar_dict(words1, words2, labels):\n",
    "    both = [word for word in words1 if word in words2]\n",
    "    words1 = [word for word in words1 if word not in both]\n",
    "    words2 = [word for word in words2 if word not in both]\n",
    "    words = [words1, words2, both]\n",
    "    labels = labels + ['Both']\n",
    "    dictionary = {}\n",
    "    for i in range(len(labels)):\n",
    "        for word in words[i]:\n",
    "            dictionary[word] = labels[i]\n",
    "    return dictionary\n",
    "\n",
    "# Converts pca model to a data frame for plotting\n",
    "def pca2df(pcamodel, embedding, dictionary):\n",
    "    # convert the pca element to a df\n",
    "    pc_df = pd.DataFrame(data = pcamodel, columns = ['pc1', 'pc2', 'pc3'])\n",
    "    # add word column to the df\n",
    "    pc_df['word'] = [key for key in embedding.wv.vocab]\n",
    "    # get a list of unique words from the dictionary\n",
    "    words = list(dictionary.keys())\n",
    "    words = list(set(words))\n",
    "    # keep only components that are in the list of unique words\n",
    "    pc_df = pc_df[pc_df['word'].isin(words)].reset_index(drop=True)\n",
    "    colors = {'Democrat': 'blue', 'Republican':'red', 'Both': 'purple', 'A': 'blue', 'B':'red'}\n",
    "    pc_df['label'] = pc_df['word'].map(dictionary)\n",
    "    pc_df['color'] = [colors[word] for word in pc_df['label']]\n",
    "    return pc_df\n",
    "\n",
    "# returns the cosine similarity of two words\n",
    "def cosine_sim(parser, keyword, text, labels):\n",
    "    # define tagged keywords. To generalize get a list of unique labels. loop through create a new variable for each label\n",
    "    keyword_r = keyword + '_r'\n",
    "    keyword_d = keyword + '_d'\n",
    "    \n",
    "    ptweets = []\n",
    "    for i in range(len(text)):\n",
    "        try:\n",
    "            ptweets.append(parser.tag_keywords(keyword, text[i], labels[i])) # tweets and labels are global variables. change to local\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('failed at '+ keyword + str(i))\n",
    "            \n",
    "\n",
    "    # lemmatize\n",
    "    ptweets = parser.multi_lemmatizer(ptweets, threads = 6)\n",
    "\n",
    "    # drop single letters\n",
    "    for i in range(len(ptweets)):\n",
    "        ptweets[i] = [word for word in ptweets[i] if len(word) > 1]\n",
    "\n",
    "    # train and save word2vec\n",
    "    pmodel = Word2Vec(ptweets, window = 10, sg = 1)\n",
    "    \n",
    "    # return cosine similarity between the words\n",
    "    return pmodel.wv.similarity(keyword_r, keyword_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "meta_data = pd.read_csv('Meta Data/meta_data.csv')\n",
    "tweet_df = pd.read_csv('Data/aggregated_tweets.csv')\n",
    "# subset to tweets after oct 29\n",
    "tweet_df = tweet_df[tweet_df['created'] >= '2019-11-06']\n",
    "# merge data with meta data\n",
    "tweet_df = pd.merge(tweet_df, meta_data, how = 'inner', on = 'user_id')\n",
    "tweet_df = tweet_df[tweet_df.party.isin(['R', 'D'])].reset_index(drop=True)\n",
    "\n",
    "tweets = tweet_df['text']\n",
    "labels = tweet_df['party']\n",
    "\n",
    "# initialize parser for both keywords and base words\n",
    "keyprep = TextPrep(stopwords = stop_words, key_words = key_words_small, key_synonyms = key_synonyms)\n",
    "baseprep = TextPrep(stopwords = stop_words, key_words = base_words, key_synonyms = base_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prep' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preprocess text\n",
    "tweets = [keyprep.twitter_preprocess(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get cosine sim for key and base words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 32s, sys: 36 s, total: 12min 8s\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get cosine similarity for all words in the key word list\n",
    "keysim = []\n",
    "for word in key_words_small[0:5]:\n",
    "    cosine = cosine_sim(parser = keyprep, keyword = word, text = tweets, labels = labels)\n",
    "    keysim.append(cosine)\n",
    "\n",
    "# Convert to dataframe\n",
    "keysimdf = pd.DataFrame(data=list(zip(key_words_small[0:5], keysim)), columns = ['word', 'similarity'])\n",
    "keysimdf.to_csv('keyword_similarity.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get cosine similarity for all words in the base word list\n",
    "basesim = []\n",
    "for word in base_words[0:5]:\n",
    "    cosine = cosine_sim(parser = baseprep, keyword = word, text = tweets, labels = labels)\n",
    "    basesim.append(cosine)\n",
    "\n",
    "# Convert to dataframe\n",
    "basesimdf = pd.DataFrame(data=list(zip(base_words[0:5], basesim)), columns = ['word', 'similarity'])\n",
    "basesimdf.to_csv('baseword_similarity.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'administration' in prep.key_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im live this morning on kfor with laceylett great talk on reducing prescription drug costs and my upcoming community conversations in oklahoma\n"
     ]
    }
   ],
   "source": [
    "if 'administration' in prep.key_synonyms:\n",
    "    print(prep.replace_synonyms('administration', tweets[0]))\n",
    "print(tweets[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
