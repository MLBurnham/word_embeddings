xbar^2/vari
}
s <- function(xbar, vari){
vari/xbar
}
# test
s(sample_mean, sample_vari)
a(sample_mean, sample_vari)
gshape <- 2
gscale <- 2
# draw sample, calculate parameters
draws <- rgamma(10000, shape = gshape, scale = gscale)
sample_mean <- mean(draws)
sample_vari <- var(draws)
# Define functions
a <- function(xbar, vari){
xbar^2/vari
}
s <- function(xbar, vari){
vari/xbar
}
# test
s(sample_mean, sample_vari)
a(sample_mean, sample_vari)
gshape <- 2
gscale <- 2
# draw sample, calculate parameters
draws <- rgamma(10000, shape = gshape, scale = gscale)
sample_mean <- mean(draws)
sample_vari <- var(draws)
# Define functions
a <- function(xbar, vari){
xbar^2/vari
}
s <- function(xbar, vari){
vari/xbar
}
# test
s(sample_mean, sample_vari)
a(sample_mean, sample_vari)
sqrt(1000)
?rnorm
pnorm(c(2,3), mean = 2.25, sd = .2598)
pnorm(c(2,3), mean = 2.25, sd = .2598, lower.tail = FALSE)
great <- pnorm(c(2), mean = 2.25, sd = .2598, lower.tail = FALSE)
less <- pnorm(c(3), mean = 2.25, sd = .2598, lower.tail = TRUE)
great - less
less - great
great <- pnorm(c(2), mean = 2.25, sd = .2598, lower.tail = FALSE)
less <- pnorm(c(3), mean = 2.25, sd = .2598, lower.tail = FALSE)
great - less
judges <- 9
p <- .25
trials <- 25
mu <- judges*p
s_sq <- mu(1-p)
s <- sqrt(s_sq)
sig <- s/sqrt(trials)
s_sq <- mu*(1-p)
s <- sqrt(s_sq)
sig <- s/sqrt(trials)
judges <- 9
p <- .25
trials <- 25
mu <- judges*p
s_sq <- mu*(1-p)
s <- sqrt(s_sq)
sig <- s/sqrt(trials)
great <- pnorm(c(2), mean = mu, sd = sig, lower.tail = FALSE)
less <- pnorm(c(3), mean = mu, sd = sig, lower.tail = FALSE)
great - less
answer <- great - less
dfm_imdb <- dfm(corpus_imdb)
library(quanteda)
library(quanteda)
library(quanteda.corpora)
install.packages(quanteda.corpora)
install.packages('quanteda.corpora')
devtools::install_github("quanteda/quanteda.corpora")
library(quanteda.corpora)
dfm_imdb <- dfm(corpus_imdb)
dfm_imdb <- dfm('corpus_imdb')
View(dfm_imdb)
dfm_imdb <- dfm(corpus_imdb)
corpus_imdb <- data_corpus_movies
dfm_imdb <- dfm(corpus_imdb)
?dfm
dfm_imdb <- dfm(corpus_imdb, remove = stopwords('english'))
?dfm_trim
dfm_imdb_sm <- dfm_trim(dfm_imdb, min_termfreq=50, mind_docfrew=5)
dfm_imdb <- dfm_trim(dfm_imdb, min_termfreq=50, mind_docfrew=5)
rm(dfm_imdb_sm)
?textmodel_wordfish
imdb_wf <- textmodel_wordfish(dfm_imdb, sparse = TRUE)
summary(imdb_wf)
?dfm
dfm_imdb <- dfm(corpus_imdb, remove = stopwords('english'), remove_punct = TRUE)
dfm_imdb <- dfm_trim(dfm_imdb, min_termfreq=50, mind_docfrew=5)
dfm_imdb <- dfm(corpus_imdb, remove = stopwords('english'), remove_punct = FALSE)
dfm_imdb <- dfm_trim(dfm_imdb, min_termfreq=50, mind_docfrew=5)
dfm_imdb <- dfm(corpus_imdb, remove = stopwords('english'), remove_punct = TRUE)
dfm_imdb <- dfm_trim(dfm_imdb, min_termfreq=50, mind_docfrew=5)
imdb_wf <- textmodel_wordfish(dfm_imdb, sparse = TRUE)
summary(imdb_wf)
library(stm)
install.packages('stm')
library(stm)
dfm_imdb_stm <- quanteda::convert(dfm_imdb_sm, to = 'stm')
?convert
dfm_imdb_stm <- quanteda::convert(dfm_imdb, to = 'stm')
?stm()
imdb_stm <- stm(documents = dfm_imdb_stm$documents, vocab = dfm_imdb_stm$vocab, k = 3, max.em.its = 75, data = dfm_imdb_stm$meta)
imdb_stm <- stm(documents = dfm_imdb_stm$documents, vocab = dfm_imdb_stm$vocab, K = 3, max.em.its = 75, data = dfm_imdb_stm$meta)
imdb_stm <- stm(documents = dfm_imdb_stm$documents, vocab = dfm_imdb_stm$vocab, K = 3, max.em.its = 75, data = dfm_imdb_stm$meta,
verbose = FALSE)
labelTopics(imdb_stm)
imdb_stm <- stm(documents = dfm_imdb_stm$documents, vocab = dfm_imdb_stm$vocab, K = 5, max.em.its = 75, data = dfm_imdb_stm$meta,
verbose = FALSE)
labelTopics(imdb_stm)
imdb_stm <- stm(documents = dfm_imdb_stm$documents, vocab = dfm_imdb_stm$vocab, K = 10, max.em.its = 75, data = dfm_imdb_stm$meta,
verbose = FALSE)
labelTopics(imdb_stm)
imdb_stm <- stm(documents = dfm_imdb_stm$documents, vocab = dfm_imdb_stm$vocab, K = 8, max.em.its = 75, data = dfm_imdb_stm$meta,
verbose = FALSE)
imdb_stm <- stm(documents = dfm_imdb_stm$documents, vocab = dfm_imdb_stm$vocab, K = 8, max.em.its = 75, data = dfm_imdb_stm$meta,
verbose = FALSE)
labelTopics(imdb_stm)
imdb_stm <- stm(documents = dfm_imdb_stm$documents, vocab = dfm_imdb_stm$vocab, K = 3, max.em.its = 75, data = dfm_imdb_stm$meta,
verbose = FALSE)
labelTopics(imdb_stm)
imdb_stm <- stm(documents = dfm_imdb_stm$documents, vocab = dfm_imdb_stm$vocab, K = 8, max.em.its = 75, data = dfm_imdb_stm$meta,
verbose = FALSE)
labelTopics(imdb_stm)
?summary
textplot_scale1d(imdb_wf, margin="features")
textplot_scale1d(imdb_wf)
?textplot_scale1d
textplot_scale1d(imdb_wf, margin ="documents")
textplot_scale1d(imdb_wf, margin="features")
imdb_stm <- stm(documents = dfm_imdb_stm$documents, vocab = dfm_imdb_stm$vocab, K = 2, max.em.its = 75, data = dfm_imdb_stm$meta,
verbose = FALSE)
labelTopics(imdb_stm)
imdb_stm <- stm(documents = dfm_imdb_stm$documents, vocab = dfm_imdb_stm$vocab, K = 8, max.em.its = 75, data = dfm_imdb_stm$meta,
verbose = FALSE)
labelTopics(imdb_stm)
pnorm(c(4,000), mean = 3750, sd = 30.61862)
pnorm(c(4000), mean = 3750, sd = 30.61862)
pnorm(c(3500), mean = 3750, sd = 30.61862, lower.tail = FALSE)
?rnorm
qnorm(.n5, mean=0, sd=1)
qnorm(.95, mean=0, sd=1)
neg_binom_var <- function(r,p{
(r*(1-p))/p^2
})
neg_binom_var <- function(r,p){
(r*(1-p))/p^2
})
#1.1
neg_binom_var <- function(r,p){
(r*(1-p))/p^2
}
neg_binom_var(r = 50, p = .05)
neg_binom_expected <- function(r,p){
(r*(1-p))/p
}
neg_binom_expected(R = 50, P = .05)
neg_binom_expected(r = 50, p = .05)
neg_binom_var(r = 50, p = .05)
binom_var <- neg_binom_var(r = 50, p = .05)
binom_sd <- sqrt(binom_var)
1.95/2
qnorm(.975, mean = 0, sd = 1)
z_score <- qnorm(.975, mean = 0, sd = 1)
sqrt(2256)
164.4854/47.49737
z_score/137/2
neg_binom_var <- function(r,p){
(r*(1-p))/p^2
}
neg_binom_expected <- function(r,p){
(r*(1-p))/p
}
neg_binom_expected(r = 50, p = .05)
binom_var <- neg_binom_var(r = 50, p = .05)
binom_sd <- sqrt(binom_var)
z_score <- qnorm(.975, mean = 0, sd = 1)
z_score/137/2
z_score/2
z_score/2*137
(z_score/2*137)^2
(z_score/2/137)^2
(z_score/2*137)
(z_score/2*137)^2
# 2*fi(c*sqrt(n)/binom_sd)-1 == .95
# fi(c*sqrt(n)/binomsd) == .975
c <- 2
# c*sqrt(n)/binom_sd == z_score
# sqrt(n)/binom_sd == z_score/c
z_score/c
# sqrt(n) == z_div_c * binom_sd
root_n <- z_div_c *binom_sd
# c*sqrt(n)/binom_sd == z_score
# sqrt(n)/binom_sd == z_score/c
z_div_c <- z_score/c
# sqrt(n) == z_div_c * binom_sd
root_n <- z_div_c *binom_sd
# n == root_n^2
root_n^2
(z_score/2*137)^2
(z_score*137/2)^2
z_score/2*137
134.2575^2
(z_score/2*137.840487520902)^2
# n == root_n^2
root_n^2
# 2.1
x <- c(-2, -.75, 1.5, .09, 1.75)
x_bar <- mean(x)
pnorm(x_bar)
pnorm(-x_bar)
pnorm(x_bar, lower.tail = FALSE)
extreme_left <- extreme_right # because the dist is symmetrical
extreme_right <- pnorm(x_bar, lower.tail = FALSE)
extreme_left <- extreme_right # because the dist is symmetrical
extreme_left_right <- extreme_right + extreme_left
?t.test()
t.test(x)
?z.test
#2.3
data(mtcars)
x <- mtcars$mpg
# Assume mpg is normal. What is the upper bound in the 95% confidence interval for the population mean
t.test(x, mu = mean(x))
len(x)
length(x)
sd <- 1
n <- length(x)
?qnorm
qnorm(0.975)
sd(x)
sd <- sd(x)
?sd
error <- qnorm(0.975)*sd/sqrt(n)
mean(x) + error
extreme_left_right <- extreme_right + extreme_left
t.test(x, mu = 0)
# 2.1
x <- c(-2, -.75, 1.5, .09, 1.75)
t.test(x, mu = 0)
# Here's what I typed in originally where I got my rounding error.
# I punched in 137 for sigma since that's what I lazily wrote on the paper rather than using the binom_sd variable I created
(z_score/2*137)^2
setwd("/home/mike/Desktop/Word Embeddings")
keysim <- read.csv('Analysis/keyword_similarity.csv')
basesim <- read.csv('Analysis/baseword_similarity.csv')
agreesim <- read.csv('Analysis/agreeword_similarity.csv')
# t-tests
t.test(keysim$similarity, basesim$similarity)
t.test(keysim$similarity, agreesim$similarity)
# libraries
library(perm)
?permTS
agreesim$label <- 'Agree'
basesim$label <- 'Base'
keysim$label <- 'Disagree'
?permKS
# permutation tests
keyagree <- bind_rows(keysim, agreesim)
library(dplyr)
# permutation tests
keyagree <- bind_rows(keysim, agreesim)
View(keyagree)
permTS(keyagree$similarity ~ keyagree$label, alternative = 'two.sided', method = 'exact.mc')
# disagree and base words
keybase <- bind_rows(keysim, basesim)
permTS(keybase$similarity ~ keybase$label, alternative = 'two.sided', method = 'exact.mc')
agreeboot <- two.boot(keysim$similarity, agreesim$similarity, mean, 1000, na.rm = TRUE)
boot.ci(agreeboot, type = 'perc')
library(simpleboot)
## Bootstraps
#
agreeboot <- two.boot(keysim$similarity, agreesim$similarity, mean, 1000, na.rm = TRUE)
boot.ci(agreeboot, type = 'perc')
library(boot)
boot.ci(agreeboot, type = 'perc')
agreeboot <- two.boot(keysim$similarity, agreesim$similarity, mean, 1000, na.rm = TRUE)
boot.ci(agreeboot, type = 'perc')
# disagree words with base words
baseboot <- two.boot(keysim$similarity, basesim$similarity, mean, 1000, na.rm = TRUE)
boot.ci(baseboot, type = 'perc')
setwd('/home/mike/Google Drive/Research Methods/pathology of propaganda')
library('foreign')
data <- read.dta('pathology_propaganda_stata15data.dta')
library(haven)
data <- read_dta('pathology_propaganda_stata15data.dta')
View(data)
View(data)
View(data)
?t.test()
data$china_rescale[data$treated ==1]
# Replicate the results from Row 1, Column 4 of Table 1 (i.e., Treated-Control). What is the value of the t-statistic used in this test?
t.test(data$china_rescale[data$treated ==1], data$china_rescale[data$treated ==0])
data$china_rescale[data$treated ==0]
library(perm)
?permTS
# Use a permutation test to test the null hypothesis of no difference in means, using the same data/variables from Question 1.
# Be sure to use at least 50,000 (50k) permutations. What is the two-tailed p-value based on this permutation test?
dv <- data$china_rescale
iv <- data$treated
permTS(dv ~ iv, alternative = 'two.sided', method = 'exact.mc')
permTS(dv ~ iv, alternative = 'two.sided', method = 'exact.mc', nmc = 50000)
permTS(dv ~ iv, alternative = 'two.sided', method = 'exact.mc', permControl(nmc = 50000))
permTS(dv ~ iv, alternative = 'two.sided', method = 'exact.mc', control = permControl(nmc = 50000))
# Replicate the results from Row 1, Column 4 of Table 1 (i.e., Treated-Control). What is the value of the t-statistic used in this test?
t.test(data$china_rescale[data$treated ==1], data$china_rescale[data$treated ==0])
# Replicate the results from Row 1, Column 4 of Table 1 (i.e., Treated-Control). What is the value of the t-statistic used in this test?
# t = -1.7884
t.test(data$china_rescale[data$treated ==0], data$china_rescale[data$treated ==1])
library(simpleboot)
?two.boot
# Use a bootstrap method to approximate the sampling distribution of the difference in means, using the same data/variables from Question 1.
# Be sure to use at least 50,000 bootstrap replications. True or false? According to this test,
# the difference in means is significantly different from 0 at the 0.05 level (two-tailed).
two.boot(data$china_rescale[data$treated ==1], data$china_rescale[data$treated ==0], mean, 50000, na.rm = TRUE)
library(boot)
# Use a bootstrap method to approximate the sampling distribution of the difference in means, using the same data/variables from Question 1.
# Be sure to use at least 50,000 bootstrap replications. True or false? According to this test,
# the difference in means is significantly different from 0 at the 0.05 level (two-tailed).
boot_test <- two.boot(data$china_rescale[data$treated ==1], data$china_rescale[data$treated ==0], mean, 50000, na.rm = TRUE)
boot.ci(boot_test, type = 'perc')
library(dplyr)
library(ggplot2)
library(tidyr)
setwd("/home/mike/Desktop/Word Embeddings")
# Read in data
# still need to subset to data after date
text <- read.csv('Data/aggregated_tweets.csv')
meta <- read.csv('Meta Data/meta_data.csv')
keysim <- read.csv('Analysis/keyword_similarity.csv')
basesim <- read.csv('Analysis/baseword_similarity.csv')
agreesim <- read.csv('Analysis/agreeword_similarity.csv')
tweets <- left_join(text, meta, by = 'user_id')
rm(text, meta)
tweets <- tweets[tweets$party == 'R' | tweets$party == 'D',]
ggtweets <- count(tweets, party)
# combine cosine data into single df
agreesim$label <- 'Agree'
basesim$label <- 'Base'
keysim$label <- 'Disagree'
cosim <- bind_rows(keysim, agreesim, basesim)
ggplot(data=ggtweets, aes(x=party, y=n, fill=party)) +
geom_bar(stat="identity", position=position_dodge(), colour="black", size = .3) +
scale_fill_manual(values = c("#0072B2", "#D55E00")) +
labs(title = "Tweets by Party", x = "Party", y = 'Tweets', fill = 'Party')+
geom_text(aes(label=n), vjust=-.3, color="black", size=4.5)+
theme_classic() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(data = cosim, aes(similarity)) + geom_density(aes(fill=factor(label)), alpha=0.8) +
labs(title="Distribution: Density Plot",
subtitle="Cosine Similarity Grouped by Category",
x="Cosine Similarity",
y="Density",
fill="Word Category")
cosimsum <- cosim %>%
group_by(label) %>%
summarise(Mean = mean(similarity), Max = max(similarity), Min = min(similarity))
box <- ggplot(data = cosim, aes(label, similarity))
box + geom_boxplot() +
geom_dotplot(binaxis='y',
stackdir='center',
dotsize = .5,
fill="red") +
theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
labs(title="Distribution: Box Plot",
subtitle="Cosine Similarity by Word Category",
x="Word Category",
y="Cosine Similarity") +
theme_bw()
keysim <- read.csv('Analysis/keyword_similarity.csv')
basesim <- read.csv('Analysis/baseword_similarity.csv')
agreesim <- read.csv('Analysis/agreeword_similarity.csv')
agreesim$label <- 'Agree'
basesim$label <- 'Base'
keysim$label <- 'Disagree'
cosim <- bind_rows(keysim, agreesim, basesim)
## t-tests
t.test(keysim$similarity, basesim$similarity)
t.test(keysim$similarity, agreesim$similarity)
keyagree <- bind_rows(keysim, agreesim)
permTS(keyagree$similarity ~ keyagree$label, alternative = 'two.sided', method = 'exact.mc')
keybase <- bind_rows(keysim, basesim)
permTS(keybase$similarity ~ keybase$label, alternative = 'two.sided', method = 'exact.mc')
agreeboot <- two.boot(keysim$similarity, agreesim$similarity, mean, 1000, na.rm = TRUE)
boot.ci(agreeboot, type = 'perc')
baseboot <- two.boot(keysim$similarity, basesim$similarity, mean, 1000, na.rm = TRUE)
boot.ci(baseboot, type = 'perc')
ggplot(data = cosim, aes(similarity)) + geom_density(aes(fill=factor(label)), alpha=0.8) +
labs(title="Distribution: Density Plot",
subtitle="Cosine Similarity Grouped by Category",
x="Cosine Similarity",
y="Density",
fill="Word Category") +
scale_fill_manual(values = c("#0072B2", "#D55E00", "#009E73"))
perm <- read.csv('Analysis/permutation.csv')
# histogram
hist(perm)
# histogram
hist(perm$cosine.similarity)
min(perm$cosine.similarity)
ggplot(perm, aes(x=cosine.similarity)) + geom_histogram()
ggplot(perm, aes(x=cosine.similarity)) +
geom_histogram(color='black', fill = '#0072B2')
ggplot(perm, aes(x=cosine.similarity)) +
geom_histogram(color='black', fill = '#0072B2') +
theme_bw()
ggplot(perm, aes(x=cosine.similarity)) +
geom_histogram(color='black', fill = '#0072B2') +
labs(title='"trump" permutation test distribution',
x = "Cosine Similarity",
y = 'Count') +
theme_bw()
ggplot(perm, aes(x=cosine.similarity)) +
geom_histogram(color='black', fill = '#0072B2') +
labs(title='Permutation Test Distribution',
subtitle='Keyword: Trump'
x = "Cosine Similarity",
y = 'Count') +
theme_bw()
ggplot(perm, aes(x=cosine.similarity)) +
geom_histogram(color='black', fill = '#0072B2') +
labs(title='Permutation Test Distribution',
subtitle='Keyword: Trump',
x = "Cosine Similarity",
y = 'Count') +
theme_bw()
ggplot(perm, aes(x=cosine.similarity)) +
geom_histogram(fill = '#0072B2') +
labs(title='Permutation Test Distribution',
subtitle='Keyword: Trump',
x = "Cosine Similarity",
y = 'Count') +
theme_bw()
ggplot(data=ggtweets, aes(x=party, y=n, fill=party)) +
geom_bar(stat="identity", position=position_dodge(), colour="black", size = .3) +
scale_fill_manual(values = c("#0072B2", "#D55E00")) +
labs(title = "Tweets by Party", x = "Party", y = 'Tweets', fill = 'Party')+
#  geom_text(aes(label=n), vjust=-.3, color="black", size=4.5)+
theme_classic() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(data=ggtweets, aes(x=party, y=n, fill=party)) +
geom_bar(stat="identity", position=position_dodge(), colour="black", size = .3) +
scale_fill_manual(values = c("#0072B2", "#D55E00")) +
labs(title = "Tweets by Party", x = "Party", y = 'Tweets', fill = 'Party')+
geom_text(aes(label=n), vjust=-.3, color="black", size=4.5)+
theme_classic()
View(perm)
View(perm)
View(keysim)
View(keysim)
View(cosim)
View(ggtweets)
View(ggtweets)
View(keyagree)
View(agreesim)
mean(agreesim$similarity)
mean(keysim$similarity)
mean(basesim$similarity)
min(basesim$similarity)
min(agreesim$similarity)
min(keysim$similarity)
max(keysim$similarity)
max(agreesim$similarity)
max(basesim$similarity)
len(keysim$similarity)
len(agreesim$similarity)
len(basesim$similarity)
length(keysim$similarity)
length(agreesim$similarity)
length(basesim$similarity)
View(keysim)
View(cosimsum)
View(cosim)
## t-tests
t.test(keysim$similarity, basesim$similarity)
t.test(keysim$similarity, agreesim$similarity)
t.test(keysim$similarity, agreesim$similarity)
## t-tests
t.test(keysim$similarity, basesim$similarity)
t.test(keysim$similarity, agreesim$similarity)
permTS(keyagree$similarity ~ keyagree$label, alternative = 'two.sided', method = 'exact.mc')
keybase <- bind_rows(keysim, basesim)
permTS(keybase$similarity ~ keybase$label, alternative = 'two.sided', method = 'exact.mc')
boot.ci(agreeboot, type = 'perc')
boot.ci(baseboot, type = 'perc')
agreeboot$t
mean(abs(agreeboot$t) > abs(agreeboot$t0)
mean(abs(agreeboot$t) > abs(agreeboot$t0))
mean(abs(baseboot$t) > abs(baseboot$t0))
baseboot$t0
baseboot$t
baseboot$t0
boot.ci(agreeboot, type = 'perc')
boot.ci(baseboot, type = 'perc')
permTS(keybase$similarity ~ keybase$label, alternative = 'two.sided', method = 'exact.mc')
boot.ci(agreeboot, type = 'perc')
boot.ci(agreeboot, type = 'perc')
boot.ci(baseboot, type = 'perc')
permTS(keyagree$similarity ~ keyagree$label, alternative = 'two.sided', method = 'exact.mc')
View(cosimsum)
View(cosim)
mean(perm$cosine.similarity)
min(perm$cosine.similarity)
