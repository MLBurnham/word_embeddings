{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for building the text pre-processing pipeline\n",
    "\n",
    "Rough outline of the pipeline:\n",
    "1. remove punctuation, numbers, capitalization from all text. Return full string rather than individual tokens\n",
    "2. isolate the key word of interest in case it appears in any hashtags\n",
    "3. replace alternatives/synonyms with a single token for that word\n",
    "4. label that token based on party affiliation\n",
    "5. remove stopwords, tokenize, lematize text\n",
    "6. train word2vec model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import demoji\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recode PrepDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepDocs:\n",
    "    \"\"\"\n",
    "    Class for either tokenizing or lemmatizing a corpus of documents\n",
    "    \n",
    "    key_tokens is a list of tokens your interested in locating in the text\n",
    "    \n",
    "    key_synonyms is meant to be a dictionary of synonyms and their associated key tokens.\n",
    "    This is helpful if used in conjunction with the replace_keyword method so that all\n",
    "    synonyms can be replaced by a single token.\n",
    "    \n",
    "    twitter_preprocess does basic cleaning for twitter text\n",
    "    \n",
    "    replace_keyword is used to replace synonyms with a single keyword\n",
    "    \n",
    "    tag_tokens will tag keywords found within a text with the appropriate label\n",
    "    \n",
    "    spacy_tokenizer and spacy_lemmatizer remove digits, punctuation, stopwords, and returns\n",
    "    either the token or lemma\n",
    "    \"\"\"\n",
    "    def __init__(self, model = 'en', stopwords = None, key_tokens = None, key_synonyms = None):\n",
    "        self.model = model\n",
    "        self.stopwords = stopwords\n",
    "        self.key_tokens = key_tokens\n",
    "        self.key_synonyms = key_words\n",
    "\n",
    "    def twitter_preprocess(self, text):\n",
    "        \"\"\"\n",
    "        This functions does some preliminary cleaning for twitter text. it will:\n",
    "        1. Remove all pound symbols from text\n",
    "        2. If key_tokens is defined, isolate key words from hashtags so they can be treated as individual tokens\n",
    "        3. Remove emoji\n",
    "        4. Remove websites from the text\n",
    "        5. Convert to lower case\n",
    "        \"\"\"\n",
    "        # remove pound signs\n",
    "        text = re.sub('#', '', text)\n",
    "        # remnove @ symbol\n",
    "        text = re.sub('@', '', text)\n",
    "        # isolate key words located in hashtags or references\n",
    "        if self.key_tokens is not None:\n",
    "            for token in self.key_tokens:\n",
    "                text = re.sub(token, ' ' + token + ' ', text)\n",
    "                text = re.sub('\\s+', ' ', text)\n",
    "                yield text\n",
    "        # remove emoji\n",
    "        text = demoji.replace(text, '')\n",
    "        # remove websites\n",
    "        text = re.sub('https:\\S*', '', text)\n",
    "        # remove excess white space between words\n",
    "        text = re.sub('\\s+', ' ', text)\n",
    "        # remove leading and trailing white space\n",
    "        text = text.strip()\n",
    "        # convert to lower case\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    \n",
    "    def replace_keyword(self, replacement, text):\n",
    "        \"\"\"\n",
    "        Finds a specific word or phrase and replaces it within a given text.\n",
    "        Generally this is used to replace synonyms for your key words of interest.\n",
    "        Use in conjunction with the key words dicitonary\n",
    "        \"\"\"\n",
    "        for key, token in self.key_synonyms.items():\n",
    "            if token == replacement:\n",
    "                word = key\n",
    "                text = re.sub(word, replacement, text)\n",
    "                text = re.sub('\\s+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tag_tokens(self, text, tag, token):\n",
    "        \"\"\"\n",
    "        Finds all instances of a user defined token within a specific corpus and tags it to\n",
    "        a specific group. Pass a list of text documents, and a list of associated tags for those documents\n",
    "        such as two columns from a data frame. \n",
    "        \"\"\"\n",
    "        text = re.sub(token, token + \"_\" + tag, text)\n",
    "        return text\n",
    "    \n",
    "    def spacy_tokenizer(self, text):\n",
    "        \"\"\"\n",
    "        Called by the clean_file method\n",
    "        Tokenizes, returns lower case, removes stop words and punctuation,\n",
    "        \"\"\"\n",
    "        nlp = spacy.load(self.model, disabled = ['tagger', 'parser', 'ner', 'textcat'])\n",
    "        # tokenize\n",
    "        text = self.nlp(text)\n",
    "        # take the token unless it is a digit, punctuation, or stopword\n",
    "        text = [token.text.lower().strip() for token in text if not token.is_digit | token.is_punct | token.is_stop]\n",
    "        # rejoin the tokens to a single string\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "\n",
    "    def spacy_lemmatizer(self, text):\n",
    "        \"\"\"\n",
    "        Called by the lemmatize_file method\n",
    "        Tokenizes, lemmatizes, returns lower case, removes stop words and punctuation\n",
    "        \"\"\"\n",
    "        # reload spacy with pat of speech tagger\n",
    "        nlp = spacy.load(self.model, disable = ['parser', 'ner', 'textcat'])\n",
    "        # tokenize\n",
    "        text = nlp(text)\n",
    "        # take the lemma unless it is a digit, punctuation, or stopword\n",
    "        text = [token.lemma_.lower().strip() for token in text if not token.is_digit | token.is_punct | token.is_stop]\n",
    "        # rejoins the tokens to a single string\n",
    "        text = \" \".join(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(self, text):\n",
    "    text = re.sub('#', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert spaces between key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_keyword(self, word, text):\n",
    "    text = re.sub(word, ' ' + word + ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key word dicitonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# a list of all key tokens\n",
    "key_tokens = ['trump',\n",
    "'president',\n",
    "'healthcare',\n",
    "'border',\n",
    "'wall',\n",
    "'democrat',\n",
    "'republican',\n",
    "'liberal',\n",
    "'conservative',\n",
    "'abortion',\n",
    "'clinton',\n",
    "'sanders',\n",
    "'socialist',\n",
    "'economy',\n",
    "'jobs',\n",
    "'impeach',\n",
    "'obama',\n",
    "'russia',\n",
    "'mueller',\n",
    "'collusion',\n",
    "'military',\n",
    "'budget',\n",
    "'market',\n",
    "'trade',\n",
    "'vote',\n",
    "'democracy',\n",
    "'gun',\n",
    "'agriculture',\n",
    "'women',\n",
    "'business',\n",
    "'tax',\n",
    "'medicare',\n",
    "'police',\n",
    "'immigration',\n",
    "'insurance',\n",
    "'climatechange',\n",
    "'corrupt',\n",
    "'electoralcollege',\n",
    "'judge',\n",
    "'court',\n",
    "'gerrymander',\n",
    "'pelosi',\n",
    "'mcconnell',\n",
    "'mikepence',\n",
    "'citizensunited',\n",
    "'daca',\n",
    "'dreamers',\n",
    "'lgbtq',\n",
    "'ACA',\n",
    "'scotus',\n",
    "'partisan',\n",
    "'patriot',\n",
    "'welfare',\n",
    "'privilege',\n",
    "'minority',\n",
    "'islam',\n",
    "'muslim',\n",
    "'christian',\n",
    "'god',\n",
    "'religion',\n",
    "'administration',\n",
    "'politics',\n",
    "'fair',\n",
    "'witchhunt',\n",
    "'warren',\n",
    "'biden',\n",
    "'security',\n",
    "'terrorism',\n",
    "'defense',\n",
    "'pentagon',\n",
    "'homelandsecurity',\n",
    "'senate',\n",
    "'wealth',\n",
    "'american',\n",
    "'church',\n",
    "'science',\n",
    "'stockmarket',\n",
    "'congress',\n",
    "'whitehouse',\n",
    "'constitution',\n",
    "'federal',\n",
    "'syria',\n",
    "'northkorea',\n",
    "'saudiarabia',\n",
    "'mexico',\n",
    "'debt',\n",
    "'fiscal',\n",
    "'oil',\n",
    "'media',\n",
    "'cnn',\n",
    "'fox',\n",
    "'news',\n",
    "'racist',\n",
    "'refugee',\n",
    "'education',\n",
    "'maga',\n",
    "'campaign',\n",
    "'party',\n",
    "'poll']\n",
    "\n",
    "f = open(\"Meta Data/key_tokens.pkl\",\"wb\")\n",
    "pickle.dump(key_tokens,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald trump\n",
      "president trump\n"
     ]
    }
   ],
   "source": [
    "# returns the synonyms for each key token\n",
    "for key, token in key_synonyms.items():\n",
    "    if token == key_tokens[0]:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-447-0f94dbff8c83>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-447-0f94dbff8c83>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    'realdonald trump': 'trump'\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# this dicitonary can be used to replace key word synonyms with a single token\n",
    "# get a list of the keys relevant, pass to the replace_keyword function\n",
    "\n",
    "key_synonyms = {\n",
    "    'donald trump': 'trump',\n",
    "    'president trump': 'trump',\n",
    "    'potus': 'trump',\n",
    "    'realdonaldtrump': 'trump'\n",
    "    'realdonald trump': 'trump'\n",
    "    'donaldtrump': 'trump'\n",
    "    'health care': 'healthcare',\n",
    "    'dems': 'democrat',\n",
    "    'gop': 'republican',\n",
    "    'left wing': 'liberal',\n",
    "    'leftist': 'liberal',\n",
    "    'progressive': 'liberal',\n",
    "    'right wing': 'conservative',\n",
    "    'abort': 'abortion',\n",
    "    'bernie sanders': 'sanders',\n",
    "    'bernie': 'sanders',\n",
    "    ' econ ': ' economy ',\n",
    "    'economics': 'economy',\n",
    "    'impeachment': 'impeach',\n",
    "    'barack obama': 'obama',\n",
    "    'robert mueller': 'mueller',\n",
    "    'armed forces': 'military',\n",
    "    'armed services': 'military',\n",
    "    'firearm': 'gun',\n",
    "    'assault rifle': 'gun',\n",
    "    'climate change': 'climatechange',\n",
    "    'global warming': 'climatechange',\n",
    "    'corruption': 'corrupt',\n",
    "    'corrupted': 'corrupt',\n",
    "    'electoral college': 'electoralcollege',\n",
    "    'court justice': 'judge',\n",
    "    'nancy pelosi': 'pelosi',\n",
    "    'mitch mcconnell': 'mcconnell',\n",
    "    'vice president mike pence': 'mikepence',\n",
    "    'vice president pence' : 'mikepence',\n",
    "    'mike pence': 'mikepence',\n",
    "    ' pence ': ' mikepence ',\n",
    "    'citizens united': 'citizensunited',\n",
    "    'deferred action for childhood arrivals': 'daca',\n",
    "    'lgbt': 'lgbtq',\n",
    "    'affordable care act': 'aca',\n",
    "    'obamacare': 'aca'\n",
    "    'obama care': 'aca'\n",
    "    'supreme court of the united states': 'scotus',\n",
    "    'us supreme court': 'scotus',\n",
    "    'united states supreme court': 'scotus',\n",
    "    'supreme court': 'scotus',\n",
    "    'islamic': 'islam',\n",
    "    'christianity': ' christian',\n",
    "    'political': 'politics',\n",
    "    'witch hunt': 'witchhunt',\n",
    "    'elizabeth warren': 'warren',\n",
    "    'counterterrorism': 'terrorism',\n",
    "    'counter-terrorism': 'terrorism',\n",
    "    'homeland security': 'homelandsecurity',\n",
    "    'us senate': 'senate',\n",
    "    'united states senate': 'senate',\n",
    "    'senatorial': 'senate',\n",
    "    'rich': 'wealthy',\n",
    "    'wealthy': ' wealth',\n",
    "    'billionaire': 'wealthy',\n",
    "    'temple': 'church',\n",
    "    'stock market': 'stockmarket',\n",
    "    'stocks': 'stockmarket',\n",
    "    'congressional': 'congress',\n",
    "    'white house': 'whitehouse',\n",
    "    'north korean': 'northkorea',\n",
    "    'north korea': 'northkorea',\n",
    "    'saudi arabian': 'saudiarabia',\n",
    "    'saudi arabia': 'saudiarabia',\n",
    "    ' saudi ': ' saudiarabia ',\n",
    "    'mexican': 'mexico',\n",
    "    'fox news': 'fox',\n",
    "    'cable news network': 'cnn',\n",
    "    'make america great again': 'maga'\n",
    "}\n",
    "\n",
    "f = open(\"Meta Data/synonym_dictionary.pkl\",\"wb\")\n",
    "pickle.dump(key_words,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_keyword(token, replacement, text):\n",
    "    text = re.sub(token, replacement, text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'president donald trump is an american hero'\n",
    "for key in list(key_words)[0:2]:\n",
    "    text = replace_keywords(key, key_words[key], text)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine stopwords list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "short list of stopwords, does not deal with contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i','a', 'about', 'am', 'an', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'how', 'in', 'is', 'it', 'of', 'on', 'or', 'that', \n",
    "             'the', 'this', 'to', 'was', 'what', 'when', 'where', 'who', 'will', 'with', 'the']\n",
    "\n",
    "f = open(\"Meta Data/stopwords.pkl\",\"wb\")\n",
    "pickle.dump(stopwords,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚Äòre',\n",
       " 'formerly',\n",
       " 'everything',\n",
       " 'while',\n",
       " 'eleven',\n",
       " 'enough',\n",
       " \"'ll\",\n",
       " 'am',\n",
       " 'seeming',\n",
       " \"n't\",\n",
       " 'hence',\n",
       " 'will',\n",
       " 'did',\n",
       " 'ever',\n",
       " 'make',\n",
       " 'with',\n",
       " 'i',\n",
       " 'whoever',\n",
       " 'what',\n",
       " 'whenever',\n",
       " 'noone',\n",
       " 'where',\n",
       " 'anyway',\n",
       " 'after',\n",
       " 'unless',\n",
       " 'n‚Äòt',\n",
       " 'everyone',\n",
       " 'done',\n",
       " 'then',\n",
       " 'each',\n",
       " 'whereafter',\n",
       " 'show',\n",
       " 'any',\n",
       " 'so',\n",
       " 'she',\n",
       " 'that',\n",
       " 'out',\n",
       " 'always',\n",
       " 'meanwhile',\n",
       " 'next',\n",
       " 'an',\n",
       " 'elsewhere',\n",
       " 'nobody',\n",
       " 'few',\n",
       " 'could',\n",
       " 'therein',\n",
       " 'does',\n",
       " 'front',\n",
       " 'himself',\n",
       " 'were',\n",
       " 'thereby',\n",
       " 'such',\n",
       " 'seems',\n",
       " 'or',\n",
       " 'yours',\n",
       " 'beside',\n",
       " 'another',\n",
       " 'quite',\n",
       " 'and',\n",
       " 'name',\n",
       " 'those',\n",
       " 'do',\n",
       " 'however',\n",
       " 'except',\n",
       " 'until',\n",
       " '‚Äôm',\n",
       " 'sixty',\n",
       " 'ca',\n",
       " 'of',\n",
       " '‚Äòs',\n",
       " 'indeed',\n",
       " 'together',\n",
       " 'many',\n",
       " 'moreover',\n",
       " 'they',\n",
       " 'around',\n",
       " \"'re\",\n",
       " '‚Äôs',\n",
       " 'too',\n",
       " 'under',\n",
       " 'via',\n",
       " 'toward',\n",
       " 'anyhow',\n",
       " 'therefore',\n",
       " 'hereupon',\n",
       " 'been',\n",
       " '‚Äòm',\n",
       " 'amongst',\n",
       " 'should',\n",
       " 'side',\n",
       " 'hers',\n",
       " 'hereby',\n",
       " 'several',\n",
       " 'when',\n",
       " 'anyone',\n",
       " 'namely',\n",
       " 'one',\n",
       " 'your',\n",
       " 'please',\n",
       " 'nothing',\n",
       " 'otherwise',\n",
       " 'here',\n",
       " 'sometimes',\n",
       " 'within',\n",
       " 'own',\n",
       " 'cannot',\n",
       " 'wherever',\n",
       " 'there',\n",
       " 'forty',\n",
       " 'over',\n",
       " 'former',\n",
       " 'used',\n",
       " 'anywhere',\n",
       " 'him',\n",
       " 'both',\n",
       " 'using',\n",
       " 'due',\n",
       " 'give',\n",
       " 'doing',\n",
       " 'all',\n",
       " 'often',\n",
       " 'you',\n",
       " 'something',\n",
       " 'less',\n",
       " 'eight',\n",
       " \"'m\",\n",
       " 'just',\n",
       " 'has',\n",
       " 'through',\n",
       " 'into',\n",
       " 'us',\n",
       " 'about',\n",
       " 'amount',\n",
       " '‚Äôll',\n",
       " 'others',\n",
       " 'along',\n",
       " 'throughout',\n",
       " 'regarding',\n",
       " 'although',\n",
       " 'call',\n",
       " 'being',\n",
       " 'else',\n",
       " '‚Äòll',\n",
       " 'rather',\n",
       " 'itself',\n",
       " 'per',\n",
       " 'between',\n",
       " 'these',\n",
       " 'herein',\n",
       " 'whole',\n",
       " 'still',\n",
       " 'as',\n",
       " 'well',\n",
       " 'whatever',\n",
       " 'be',\n",
       " 'whereas',\n",
       " 'can',\n",
       " 'above',\n",
       " 'seem',\n",
       " 'becoming',\n",
       " 'ten',\n",
       " 'but',\n",
       " 'become',\n",
       " 'at',\n",
       " 'almost',\n",
       " 'three',\n",
       " 're',\n",
       " 'below',\n",
       " 'whether',\n",
       " 'how',\n",
       " 'third',\n",
       " 'ourselves',\n",
       " 'other',\n",
       " 'them',\n",
       " 'also',\n",
       " 'their',\n",
       " 'seemed',\n",
       " 'me',\n",
       " 'same',\n",
       " 'yourself',\n",
       " 'may',\n",
       " 'against',\n",
       " 'last',\n",
       " 'thereupon',\n",
       " 'empty',\n",
       " 'why',\n",
       " 'thus',\n",
       " 'alone',\n",
       " 'hereafter',\n",
       " 'was',\n",
       " 'bottom',\n",
       " 'hundred',\n",
       " 'the',\n",
       " 'afterwards',\n",
       " 'now',\n",
       " 'themselves',\n",
       " 'its',\n",
       " 'nowhere',\n",
       " '‚Äòd',\n",
       " 'some',\n",
       " 'fifteen',\n",
       " 'least',\n",
       " 'everywhere',\n",
       " 'say',\n",
       " 'see',\n",
       " 'among',\n",
       " 'again',\n",
       " 'whence',\n",
       " 'yet',\n",
       " 'ours',\n",
       " 'every',\n",
       " 'not',\n",
       " 'on',\n",
       " '‚Äôre',\n",
       " 'by',\n",
       " '‚Äôve',\n",
       " 'a',\n",
       " 'we',\n",
       " 'thru',\n",
       " 'four',\n",
       " 'full',\n",
       " 'nor',\n",
       " 'myself',\n",
       " 'somehow',\n",
       " 'in',\n",
       " \"'d\",\n",
       " '‚Äôd',\n",
       " 'whither',\n",
       " 'without',\n",
       " 'herself',\n",
       " 'his',\n",
       " 'various',\n",
       " 'it',\n",
       " 'either',\n",
       " 'neither',\n",
       " 'had',\n",
       " 'sometime',\n",
       " 'more',\n",
       " 'mostly',\n",
       " 'back',\n",
       " 'off',\n",
       " 'across',\n",
       " 'mine',\n",
       " 'to',\n",
       " 'my',\n",
       " 'became',\n",
       " 'her',\n",
       " 'beyond',\n",
       " 'take',\n",
       " 'which',\n",
       " 'upon',\n",
       " 'top',\n",
       " 'yourselves',\n",
       " 'go',\n",
       " 'latter',\n",
       " 'even',\n",
       " 'might',\n",
       " 'who',\n",
       " 'once',\n",
       " 'must',\n",
       " 'nine',\n",
       " 'five',\n",
       " 'anything',\n",
       " 'are',\n",
       " 'really',\n",
       " 'six',\n",
       " 'because',\n",
       " 'someone',\n",
       " 'is',\n",
       " 'get',\n",
       " 'somewhere',\n",
       " 'two',\n",
       " 'becomes',\n",
       " 'whereupon',\n",
       " 'very',\n",
       " 'keep',\n",
       " 'never',\n",
       " 'move',\n",
       " 'serious',\n",
       " 'thence',\n",
       " 'up',\n",
       " 'no',\n",
       " 'down',\n",
       " 'twenty',\n",
       " 'though',\n",
       " 'he',\n",
       " 'for',\n",
       " 'perhaps',\n",
       " 'if',\n",
       " 'whose',\n",
       " 'nevertheless',\n",
       " 'since',\n",
       " 'behind',\n",
       " 'from',\n",
       " 'only',\n",
       " 'much',\n",
       " 'latterly',\n",
       " 'twelve',\n",
       " '‚Äòve',\n",
       " \"'s\",\n",
       " 'none',\n",
       " 'wherein',\n",
       " 'n‚Äôt',\n",
       " \"'ve\",\n",
       " 'further',\n",
       " 'before',\n",
       " 'would',\n",
       " 'than',\n",
       " 'first',\n",
       " 'during',\n",
       " 'have',\n",
       " 'besides',\n",
       " 'most',\n",
       " 'made',\n",
       " 'this',\n",
       " 'our',\n",
       " 'beforehand',\n",
       " 'onto',\n",
       " 'whom',\n",
       " 'fifty',\n",
       " 'put',\n",
       " 'already',\n",
       " 'towards',\n",
       " 'thereafter',\n",
       " 'whereby']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longer list. spacy's default list of stop words minus a few key terms that may be relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "# Stop words\n",
    "STOP_WORDS = set(\n",
    "    \"\"\"\n",
    "a about after afterwards again all almost\n",
    "already also although always am among amongst amount an and another any anyhow\n",
    "anyone anything anyway anywhere are around as at\n",
    "\n",
    "be became because become becomes becoming been before beforehand\n",
    "being beside besides beyond both bottom but by\n",
    "\n",
    "can cannot ca could\n",
    "\n",
    "did do does doing done down during\n",
    "\n",
    "each eight either eleven else elsewhere empty enough even ever every\n",
    "everyone everything everywhere except\n",
    "\n",
    "few fifteen fifty first five for former formerly forty four from front full\n",
    "further\n",
    "\n",
    "get go\n",
    "\n",
    "had has have he hence her here hereafter hereby herein hereupon hers herself\n",
    "him himself his how however hundred\n",
    "\n",
    "i if in indeed into is it its itself\n",
    "\n",
    "last latter latterly least less\n",
    "\n",
    "many may me meanwhile might mine more moreover most mostly move much\n",
    "must my myself\n",
    "\n",
    "name namely neither nevertheless next nine no nobody none noone nor not\n",
    "nothing now nowhere\n",
    "\n",
    "of off often on once one only onto or other others otherwise our ours ourselves\n",
    "out over own\n",
    "\n",
    "part per perhaps please put\n",
    "\n",
    "quite\n",
    "\n",
    "rather re really regarding\n",
    "\n",
    "same say see seem seemed seeming seems serious several she should show side\n",
    "since six sixty so some somehow someone something sometime sometimes somewhere\n",
    "still such\n",
    "\n",
    "take ten than that the their them themselves then thence there thereafter\n",
    "thereby therefore therein thereupon these they third this those though three\n",
    "through throughout thru thus to together too top toward towards twelve twenty\n",
    "two\n",
    "\n",
    "under until up unless upon used using\n",
    "\n",
    "various very very via was we well were what whatever when whence whenever where\n",
    "\n",
    "whereafter whereas whereby wherein whereupon wherever whether which while\n",
    "whither who whoever whole whom whose why will with within without would\n",
    "\n",
    "yet you your yours yourself yourselves\n",
    "\"\"\".split()\n",
    ")\n",
    "\n",
    "contractions = [\"n't\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\"]\n",
    "STOP_WORDS.update(contractions)\n",
    "\n",
    "for apostrophe in [\"‚Äò\", \"‚Äô\"]:\n",
    "    for stopword in contractions:\n",
    "        STOP_WORDS.add(stopword.replace(\"'\", apostrophe))\n",
    "        \n",
    "f = open(\"Meta Data/stopwords.pkl\",\"wb\")\n",
    "pickle.dump(STOP_WORDS,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Lemmas of key tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>constitution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>federal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>syria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>north korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>saudi arabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>gop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>debt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>fiscal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>refugee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>maga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>mitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>campaign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>poll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "80  constitution\n",
       "81       federal\n",
       "82         syria\n",
       "83   north korea\n",
       "84  saudi arabia\n",
       "85           gop\n",
       "86        mexico\n",
       "87          debt\n",
       "88        fiscal\n",
       "89           oil\n",
       "90         media\n",
       "91          news\n",
       "92        racist\n",
       "93       refugee\n",
       "94     education\n",
       "95          maga\n",
       "96         mitch\n",
       "97      campaign\n",
       "98         party\n",
       "99          poll"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = pd.read_csv('Political terms.csv', header = None)\n",
    "terms.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump\n",
      "president\n",
      "healthcare\n",
      "border\n",
      "wall\n",
      "democrat\n",
      "republican\n",
      "liberal\n",
      "conservative\n",
      "abortion abort\n",
      "clinton\n",
      "sander\n",
      "socialist\n",
      "economy\n",
      "job\n",
      "impeachment\n",
      "potus\n",
      "obama\n",
      "russia\n",
      "mueller\n",
      "collusion\n",
      "military\n",
      "budget\n",
      "market\n",
      "trade\n",
      "vote\n",
      "democracy\n",
      "gun\n",
      "firearm\n",
      "assualt rifle\n",
      "agriculture\n",
      "woman\n",
      "business\n",
      "tax\n",
      "medicare\n",
      "police\n",
      "immigration\n",
      "insurance\n",
      "climate\n",
      "corrupt\n",
      "progressive\n",
      "electoral college\n",
      "judge\n",
      "court\n",
      "gerrymander\n",
      "pelosi\n",
      "mcconnell\n",
      "penny\n",
      "citizen unite\n",
      "dreamer\n",
      "lgbtq\n",
      "obamacare\n",
      "scotus\n",
      "partisan\n",
      "patriot\n",
      "welfare\n",
      "privilege\n",
      "minority\n",
      "muslim\n",
      "god\n",
      "religion\n",
      "administration\n",
      "politic\n",
      "\n",
      "fair\n",
      "witch hunt\n",
      "warren\n",
      "biden\n",
      "security\n",
      "terrorism\n",
      "pentagon\n",
      "senate\n",
      "rich\n",
      "american\n",
      "church\n",
      "science\n",
      "supreme court\n",
      "stock\n",
      "congress\n",
      "white house\n",
      "constitution\n",
      "federal\n",
      "syria\n",
      "north korea\n",
      "saudi arabia\n",
      "gop\n",
      "mexico\n",
      "debt\n",
      "fiscal\n",
      "oil\n",
      "medium\n",
      "news\n",
      "racist\n",
      "refugee\n",
      "education\n",
      "maga\n",
      "mitch\n",
      "campaign\n",
      "party\n",
      "poll\n"
     ]
    }
   ],
   "source": [
    "for i in terms[0]:\n",
    "    print(prep.spacy_lemmatizer(text = i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key token tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"find out about how #donald trump lied at https://t.co/asdfk!lasd08\", 'this is second test sentance trump', \n",
    "        'did you watch the trump impeachment hearings today?']\n",
    "tag_list = ['D', \"R\", \"D\"]\n",
    "\n",
    "def tag_tokens(text_list, token, tag_list):\n",
    "    for i in range(len(text_list)):\n",
    "        text_list[i] = re.sub(token, token + \"_\" + tag_list[i], text_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['find out about how #donald trump_D lied at https://t.co/asdfk!lasd08',\n",
       " 'this is second test sentance trump_R',\n",
       " 'did you watch the trump_D impeachment hearings today?']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_tokens(text, 'trump', tag_list)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_tweets = pd.read_csv('Data/aggregated_tweets.csv')\n",
    "text = aggregated_tweets['text']\n",
    "text[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'At 123145 what point did due process become \"cosmetic\" in this country? I don\\'t remember learning that in law school. ü§î https://t.co/ew1hbvbWBj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At what point did due process become \"cosmetic\" in this country? I don\\'t remember learning that in law school.  https://t.co/ew1hbvbWBj'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demoji.replace(test, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find out about how @realdonald trump lied about obama care at \n"
     ]
    }
   ],
   "source": [
    "test_tokens = ['trump', 'token']\n",
    "text = \"find out about how @realdonaldtrump lied about obamacare at https://t.co/asdfk!lasd08\"\n",
    "tester = 1\n",
    "def twitter_preprocess(text):\n",
    "    \"\"\"\n",
    "    This functions does some preliminary cleaning for twitter text. it will:\n",
    "    1. Remove all pound symbols from text\n",
    "    2. Isolate key words located within hashtags so they can be treated as individual tokens\n",
    "    3. Remove websites from the text\n",
    "    \"\"\"\n",
    "    # remove pound signs\n",
    "    text = re.sub('#', '', text)\n",
    "    # isolate key words located in hashtags\n",
    "    if tester is not None:\n",
    "        for token in key_tokens:\n",
    "            text = re.sub(token, ' ' + token + ' ', text)\n",
    "            text = re.sub('\\s+', ' ', text)\n",
    "    # remove websites\n",
    "    text = re.sub('https:\\S*', '', text)\n",
    "    print(text)\n",
    "\n",
    "twitter_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en', disable = ['tagger', 'parser', 'ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[point,\n",
       " process,\n",
       " cosmetic,\n",
       " country,\n",
       " remember,\n",
       " learning,\n",
       " law,\n",
       " school,\n",
       " ü§î,\n",
       " https://t.co/ew1hbvbWBj]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_strip = tokenizer(test)\n",
    "\n",
    "test_strip = [token for token in test_strip if not token.is_digit | token.is_punct | token.is_stop]\n",
    "test_strip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "file = open('Meta Data/key_tokens.pkl', 'rb')\n",
    "key_tokens = pickle.load(file)\n",
    "\n",
    "file = open('Meta Data/synonym_dictionary.pkl', 'rb')\n",
    "synonyms = pickle.load(file)\n",
    "\n",
    "file = open('Meta Data/stopwords.pkl', 'rb')\n",
    "stopwords = pickle.load(file)\n",
    "\n",
    "meta_data = pd.read_csv('Meta Data/meta_data.csv')\n",
    "\n",
    "tweet_df = pd.read_csv('Data/aggregated_tweets.csv')\n",
    "\n",
    "\n",
    "# merge data with meta data\n",
    "tweet_df = pd.merge(tweet_df, meta_data, how = 'inner', on = 'user_id')\n",
    "\n",
    "tweets = tweet_df['text']\n",
    "labels = tweet_df['party']\n",
    "\n",
    "# initialize parser\n",
    "prep = PrepDocs(stopwords = stopwords, key_tokens = key_tokens, key_synonyms = synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68 ms, sys: 0 ns, total: 68 ms\n",
      "Wall time: 66.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweets = [prep.twitter_preprocess(tweet) for tweet in tweets[0:20]]\n",
    "#tweets = [prep.spacy_lemmatizer(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that didn‚Äôt take long. after a vote to make the impeach ment inquiry ‚Äúopen and transparent‚Äù today (h. res. 660), i learn of numerous closed depositions taking place next week. that‚Äôs surprising. stoptheschiffshow',\n",
       " 'looking forward to showing repmarkgreen around fl15 next week &amp; introducing him to some outstanding veterans in our community! florida',\n",
       " 'a yes vote on this resolution gives a stamp of approval to a process that has been damaged beyond repair. watch my full floor speech from this morning below. florida fl15 impeachmentinquiry',\n",
       " 'the house will vote on a resolution that will formalize the impeachmentinquiry into realdonaldtrump. but, the damage is already done. it is clear that democrats are not interested in precedent or due process. they have one goal ‚Äì to remove the potus from office. period.',\n",
       " 'my dc team and i were happy to help the_uso this afternoon! in total, 2000 care packages were assembled for our deployed service members. thank you for defending our freedom and to the uso for being a force behind our armed forces! fl15 fl',\n",
       " 'thank you kelliemeyernews for interviewing me today! look out for my interview about the boeing 737 max hearing on wfla this evening. florida fl15 transportgop',\n",
       " 'i just stepped out of the house t&amp;i committee hearing regarding the boeing 737 max for a brief moment. you can watch the live feed here: transportgop',\n",
       " \"20. that's the number of legislative days left to fully fund our military . yet, democrats are laser focused on impeach ment instead. where do their priorities lie?\",\n",
       " 'it‚Äôs too little, too late. the democrat‚Äôs motives have already been revealed. we will not validate them.',\n",
       " \"i'm grateful that my colleagues &amp; i could work together to get polkschoolsnews the assistance they needed. thank you repdarrensoto &amp; repgregsteube for this outstanding bi partisan effort! fl15 florida\",\n",
       " 'update - we have stepped out of the house intelligence comm. meeting to vote . unfortunately, chairman schiff im media tely stopped the hearing &amp; left with the witness upon our arrival. stoptheschiffshow',\n",
       " 'even losing my voice wasn‚Äôt going to stop me from drawing attention to this important issue. this impeach ment inquiry has been nothing short of a mess. the process the democrats are leading has been unilateral and secretive. it‚Äôs unacceptable. stoptheschiffshow',\n",
       " 'all we want is transparency. plain and simple. how can we make well-informed decisions if we don‚Äôt have access to all of the info? americans deserve transparency. stoptheschiffshow',\n",
       " \"we at the republicanstudy are working to develop a personalized plan. as someone with a preexisting condition, i know the importance of having a meaningful healthcare plan that isn't one-sized-fits-all. healthcare4u\",\n",
       " \"ashley hall was a mother of two young girls, a daughter, a sister, a dance instructor and a teacher's aid. she was also a victim of domestic violence. this domesticviolenceawarenessmonth, let ashley's story and others be a motivator to get help and give help. fl15 florida\",\n",
       " 'thank you, orlandosentinel for publishing this! no one should have to worry about unexpected medical bills, especially if they are already insured. i am glad to join senrickscott in continuing to address this increasingly common issue. fl15 florida',\n",
       " 'accountability is key here. lying to the american people must be condemned.',\n",
       " 'we are praying for those impacted by the storms that hit polkcounty. as we‚Äôve done before, we will rebuild stronger and better than ever. fl15 florida',\n",
       " 'from the impeach ment process, to the topic surrounding our kurdish allies, and fighting for the issues of veterans and fl15, it has been quite the hectic week. catch up on all that has been happening in washington in my weekly recap video below! florida',\n",
       " 'thanks to the sorenson family from lakeland for visiting us today! it‚Äôs always nice to connect with the folks from home when i am in washington. if you and your family are interested in touring the capitol, reach out to my dc office and we can arrange it! fl15 florida']"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at 123145 what point did due process become \"cosmetic\" in this country? trump_D i dont remember that in law school. trump_D'"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = 'At 123145 what point did due process become \"cosmetic\" in this country? #presidenttrump I dont remember that in law school. ü§î https://t.co/ew1hbvbWBj Donald Trump'\n",
    "test_text = prep.twitter_preprocess(test_text)\n",
    "key_token = prep.key_tokens[0]\n",
    "test_text = prep.replace_keyword(key_token, test_text)\n",
    "test_text = prep.tag_tokens(test_text, 'D', key_token)\n",
    "\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text_list)):\n",
    "    text_list[i] = re.sub(token, token + \"_\" + tag_list[i], text_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trump',\n",
       " 'president',\n",
       " 'healthcare',\n",
       " 'border',\n",
       " 'wall',\n",
       " 'democrat',\n",
       " 'republican',\n",
       " 'liberal',\n",
       " 'conservative',\n",
       " 'abortion',\n",
       " 'clinton',\n",
       " 'sanders',\n",
       " 'socialist',\n",
       " 'economy',\n",
       " 'jobs',\n",
       " 'impeach',\n",
       " 'obama',\n",
       " 'russia',\n",
       " 'mueller',\n",
       " 'collusion',\n",
       " 'military',\n",
       " 'budget',\n",
       " 'market',\n",
       " 'trade',\n",
       " 'vote',\n",
       " 'democracy',\n",
       " 'gun',\n",
       " 'agriculture',\n",
       " 'women',\n",
       " 'business',\n",
       " 'tax',\n",
       " 'medicare',\n",
       " 'police',\n",
       " 'immigration',\n",
       " 'insurance',\n",
       " 'climatechange',\n",
       " 'corrupt',\n",
       " 'electoralcollege',\n",
       " 'judge',\n",
       " 'court',\n",
       " 'gerrymander',\n",
       " 'pelosi',\n",
       " 'mcconnell',\n",
       " 'mikepence',\n",
       " 'citizensunited',\n",
       " 'daca',\n",
       " 'dreamers',\n",
       " 'lgbtq',\n",
       " 'ACA',\n",
       " 'scotus',\n",
       " 'partisan',\n",
       " 'patriot',\n",
       " 'welfare',\n",
       " 'privilege',\n",
       " 'minority',\n",
       " 'islam',\n",
       " 'muslim',\n",
       " 'christian',\n",
       " 'god',\n",
       " 'religion',\n",
       " 'administration',\n",
       " 'politics',\n",
       " 'fair',\n",
       " 'witchhunt',\n",
       " 'warren',\n",
       " 'biden',\n",
       " 'security',\n",
       " 'terrorism',\n",
       " 'defense',\n",
       " 'pentagon',\n",
       " 'homelandsecurity',\n",
       " 'senate',\n",
       " 'wealth',\n",
       " 'american',\n",
       " 'church',\n",
       " 'science',\n",
       " 'stockmarket',\n",
       " 'congress',\n",
       " 'whitehouse',\n",
       " 'constitution',\n",
       " 'federal',\n",
       " 'syria',\n",
       " 'northkorea',\n",
       " 'saudiarabia',\n",
       " 'mexico',\n",
       " 'debt',\n",
       " 'fiscal',\n",
       " 'oil',\n",
       " 'media',\n",
       " 'cnn',\n",
       " 'fox',\n",
       " 'news',\n",
       " 'racist',\n",
       " 'refugee',\n",
       " 'education',\n",
       " 'maga',\n",
       " 'campaign',\n",
       " 'party',\n",
       " 'poll']"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.key_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
