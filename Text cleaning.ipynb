{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for building the text pre-processing pipeline\n",
    "\n",
    "Rough outline of the pipeline:\n",
    "1. remove punctuation, numbers, capitalization from all text. Return full string rather than individual tokens\n",
    "2. isolate the key word of interest in case it appears in any hashtags\n",
    "3. replace alternatives/synonyms with a single token for that word\n",
    "4. label that token based on party affiliation\n",
    "5. tokenize/lematize text\n",
    "6. train word2vec model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recode PrepDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fa407fa7438>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.load('en', disable = ['tagger', 'parser', 'ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "class PrepDocs:\n",
    "    \"\"\"\n",
    "    Class for either tokenizing or lemmatizing a corpus of documents\n",
    "\n",
    "    self.clean_text will do a basic cleaning that returns lower case, removes\n",
    "    stop words, and removes punctuation\n",
    "\n",
    "    slef.lemmatize_text will do the basic cleaning plus lemmatization\n",
    "\n",
    "    input_dir: directory where text documents to be cleaned are stored\n",
    "    output_dir: directory where cleans documents are to be saved\n",
    "    \"\"\"\n",
    "    def __init__(self, model = 'en', disabled = ['tagger', 'parser', 'ner', 'textcat']):\n",
    "        self.nlp = spacy.load(model, disable = disabled)\n",
    "        self.stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "        self.stopwords.remove()\n",
    "        self.not_allowed = string.punctuation + string.digits\n",
    "\n",
    "    def spacy_tokenizer(self, text):\n",
    "        \"\"\"\n",
    "        Called by the clean_file method\n",
    "        Tokenizes, returns lower case, removes stop words and punctuation,\n",
    "        \"\"\"\n",
    "        # drop subsection numbers\n",
    "        text = re.sub(r\"\\d\\.\", \"\", text)\n",
    "        # tokenize\n",
    "        text = self.nlp(text)\n",
    "        # take the lemma unless it is a pronoun\n",
    "        text = [tok.text.lower().strip() for tok in text]\n",
    "        # drop digits\n",
    "        text = [token for token in text if not token.isdigit()]\n",
    "        # drop stopwords and punctuation\n",
    "        text = [tok for tok in text if (tok not in self.stopwords and tok not in self.not_allowed)]\n",
    "        # rejoin the tokens to a single string\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "\n",
    "    def clean_file(self, doc):\n",
    "        \"\"\"\n",
    "        Used for cleaning a .txt file\n",
    "        \"\"\"\n",
    "        text = open(doc, errors = \"ignore\").read()\n",
    "        text = self.spacy_tokenizer(text)\n",
    "        text_file = open(self.output_dir + \"/\" + doc[0:-4] + '_clean' '.txt',\n",
    "                             'w', errors='replace')\n",
    "        text_file.write(text)\n",
    "\n",
    "    def spacy_lemmatizer(self, text):\n",
    "        \"\"\"\n",
    "        Called by the lemmatize_file method\n",
    "        Tokenizes, lemmatizes, returns lower case, removes stop words and punctuation\n",
    "        \"\"\"\n",
    "        # reload spacy with pat of speech tagger\n",
    "        nlp = spacy.load('en', disable = ['parser', 'ner', 'textcat'])\n",
    "        # tokenize\n",
    "        text = nlp(text)\n",
    "        # drop stopwords\n",
    "        # take the lemma unless it is a stopword\n",
    "        text = [tok.lemma_.lower().strip() for tok in text if (tok.text not in self.stopwords and tok.text not in self.not_allowed)]\n",
    "        # drop digits\n",
    "        text = [token for token in text if not token.isdigit()]\n",
    "        # rejoins the tokens to a single string\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "    \n",
    "    def remove_hashtags(self, text):\n",
    "        \"\"\"\n",
    "        Removes hashtags from text. The spacy lemmatizer will do this on its own.\n",
    "        Use this if you want to remove hashtags without taking the lemma\n",
    "        \"\"\"\n",
    "        text = re.sub('#', '', text)\n",
    "        return text\n",
    "    \n",
    "    def isolate_keyword(self, word, text):\n",
    "        \"\"\"\n",
    "        Inserts spaces between key words and then strips excess white space so that\n",
    "        key words are treated as individual tokens. Particularly useful for isolating\n",
    "        key words that are found in hashtags\n",
    "        \"\"\"\n",
    "        text = re.sub(word, ' ' + word + ' ', text)\n",
    "        text = re.sub('\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def lemmatize_file(self, doc):\n",
    "        \"\"\"\n",
    "        Used for lemmatizing a .txt file\n",
    "        \"\"\"\n",
    "        text = open(self.input_dir + \"/\" + doc, errors = \"ignore\").read()\n",
    "        text = self.spacy_lemmatizer(text)\n",
    "        text_file = open(self.output_dir + \"/\" + doc[0:-4] + '_clean' '.txt',\n",
    "                             'w', errors='replace')\n",
    "        text_file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(self, text):\n",
    "    text = re.sub('#', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert spaces between key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_keyword(self, word, text):\n",
    "    text = re.sub(word, ' ' + word + ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key word dicitonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "key_words = {\n",
    "    'donald trump': 'trump',\n",
    "    'president trump': 'trump',\n",
    "    'health care': 'healthcare',\n",
    "    'dems': 'democrat',\n",
    "    'gop': 'republican',\n",
    "    'left wing': 'liberal',\n",
    "    'leftist': 'liberal',\n",
    "    'progressive': 'liberal',\n",
    "    'right wing': 'conservative',\n",
    "    'abort': 'abortion',\n",
    "    'bernie sanders': 'sanders',\n",
    "    'bernie': 'sanders',\n",
    "    ' econ ': ' economy ',\n",
    "    'economics': 'economy',\n",
    "    'impeachment': 'impeach',\n",
    "    'barack obama': 'obama',\n",
    "    'robert mueller': 'mueller',\n",
    "    'armed forces': 'military',\n",
    "    'armed services': 'military',\n",
    "    'firearm': 'gun',\n",
    "    'assault rifle': 'gun',\n",
    "    'climate change': 'climatechange',\n",
    "    'global warming': 'climatechange',\n",
    "    'corruption': 'corrupt',\n",
    "    'corrupted': 'corrupt',\n",
    "    'electoral college': 'electoralcollege',\n",
    "    'court justice': 'judge',\n",
    "    'nancy pelosi': 'pelosi',\n",
    "    'mitch mcconnell': 'mcconnell',\n",
    "    'vice president mike pence': 'mikepence',\n",
    "    'vice president pence' : 'mikepence',\n",
    "    'mike pence': 'mikepence',\n",
    "    ' pence ': ' mikepence ',\n",
    "    'citizens united': 'citizensunited',\n",
    "    'deferred action for childhood arrivals': 'daca',\n",
    "    'lgbt': 'lgbtq',\n",
    "    'affordable care act': 'aca',\n",
    "    'supreme court of the united states': 'scotus',\n",
    "    'us supreme court': 'scotus',\n",
    "    'united states supreme court': 'scotus',\n",
    "    'supreme court': 'scotus',\n",
    "    'islamic': 'islam',\n",
    "    'christianity': ' christian',\n",
    "    'political': 'politics',\n",
    "    'witch hunt': 'witchhunt',\n",
    "    'elizabeth warren': 'warren',\n",
    "    'counterterrorism': 'terrorism',\n",
    "    'counter-terrorism': 'terrorism',\n",
    "    'homeland security': 'homelandsecurity',\n",
    "    'us senate': 'senate',\n",
    "    'united states senate': 'senate',\n",
    "    'senatorial': 'senate',\n",
    "    'rich': 'wealthy',\n",
    "    'wealthy': ' wealth',\n",
    "    'billionaire': 'wealthy',\n",
    "    'temple': 'church',\n",
    "    'stock market': 'stockmarket',\n",
    "    'stocks': 'stockmarket',\n",
    "    'congressional': 'congress',\n",
    "    'white house': 'whitehouse',\n",
    "    'north korean': 'northkorea',\n",
    "    'north korea': 'northkorea',\n",
    "    'saudi arabian': 'saudiarabia',\n",
    "    'saudi arabia': 'saudiarabia',\n",
    "    'saudi': 'saudiarabia',\n",
    "    'mexican': 'mexico',\n",
    "    'fox news': 'fox',\n",
    "    'cable news network': 'cnn',\n",
    "    'make america great again': 'maga'\n",
    "}\n",
    "\n",
    "f = open(\"Meta Data/replacement_dictionary.pkl\",\"wb\")\n",
    "pickle.dump(key_words,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wealthy state'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = PrepDocs('none', 'none')\n",
    "prep.spacy_lemmatizer(text = 'the wealthy state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following terms need to be converted via regular expressions:\n",
    "1. synonyms should be converted to a single term, words that won't be located in both the congressional and troll data set should be eliminated In order to catch compound words using my key tokens such as \"moscowmitch\" replace each \"token\" with \" token \" via regular expression\n",
    "\n",
    "2. Remove hastags\n",
    "\n",
    "\n",
    "impeachment > impeach\n",
    "\n",
    "bernie sanders > sanders\n",
    "\n",
    "bernie > sanders\n",
    "\n",
    "abort > abortion\n",
    "\n",
    "global warming > climate change\n",
    "\n",
    "climate change > climatechange\n",
    "\n",
    "electoral college > electoralcollege\n",
    "\n",
    "pence > mikepence\n",
    "\n",
    "mike pence > mikepence\n",
    "\n",
    "citizens united > citizensunited\n",
    "\n",
    "lgbt > lgbtq\n",
    "\n",
    "gay > lgbtq\n",
    "\n",
    "lesbian > lgbtq\n",
    "\n",
    "homosexual > lgbtq\n",
    "\n",
    "supreme court > scotus\n",
    "\n",
    "witch hunt > witchhunt\n",
    "\n",
    "north korea > northkorea\n",
    "\n",
    "saudi arabia > saudi\n",
    "\n",
    "media > newsmedia\n",
    "\n",
    "corruption > corrupt\n",
    "\n",
    "corrupted > corrupt\n",
    "\n",
    "obamacare > ACA\n",
    "\n",
    "affordable care act > ACA\n",
    "\n",
    "donald trump > trump\n",
    "\n",
    "assault rifle > gun\n",
    "\n",
    "firearm > gun\n",
    "\n",
    "potus > president\n",
    "\n",
    "islamic > islam\n",
    "\n",
    "racism > racist\n",
    "\n",
    "mitch mcconnell > mitchmcconnell\n",
    "\n",
    "mcconnell > mitchmcconnell\n",
    "\n",
    "nancy pelosi > nancypelosi\n",
    "\n",
    "pelosi > nancypelosi\n",
    "\n",
    "taxing > tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>constitution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>federal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>syria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>north korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>saudi arabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>gop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>debt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>fiscal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>refugee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>maga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>mitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>campaign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>poll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "80  constitution\n",
       "81       federal\n",
       "82         syria\n",
       "83   north korea\n",
       "84  saudi arabia\n",
       "85           gop\n",
       "86        mexico\n",
       "87          debt\n",
       "88        fiscal\n",
       "89           oil\n",
       "90         media\n",
       "91          news\n",
       "92        racist\n",
       "93       refugee\n",
       "94     education\n",
       "95          maga\n",
       "96         mitch\n",
       "97      campaign\n",
       "98         party\n",
       "99          poll"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = pd.read_csv('Political terms.csv', header = None)\n",
    "terms.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump\n",
      "president\n",
      "healthcare\n",
      "border\n",
      "wall\n",
      "democrat\n",
      "republican\n",
      "liberal\n",
      "conservative\n",
      "abortion abort\n",
      "clinton\n",
      "sander\n",
      "socialist\n",
      "economy\n",
      "job\n",
      "impeachment\n",
      "potus\n",
      "obama\n",
      "russia\n",
      "mueller\n",
      "collusion\n",
      "military\n",
      "budget\n",
      "market\n",
      "trade\n",
      "vote\n",
      "democracy\n",
      "gun\n",
      "firearm\n",
      "assualt rifle\n",
      "agriculture\n",
      "woman\n",
      "business\n",
      "tax\n",
      "medicare\n",
      "police\n",
      "immigration\n",
      "insurance\n",
      "climate\n",
      "corrupt\n",
      "progressive\n",
      "electoral college\n",
      "judge\n",
      "court\n",
      "gerrymander\n",
      "pelosi\n",
      "mcconnell\n",
      "penny\n",
      "citizen unite\n",
      "dreamer\n",
      "lgbtq\n",
      "obamacare\n",
      "scotus\n",
      "partisan\n",
      "patriot\n",
      "welfare\n",
      "privilege\n",
      "minority\n",
      "muslim\n",
      "god\n",
      "religion\n",
      "administration\n",
      "politic\n",
      "\n",
      "fair\n",
      "witch hunt\n",
      "warren\n",
      "biden\n",
      "security\n",
      "terrorism\n",
      "pentagon\n",
      "senate\n",
      "rich\n",
      "american\n",
      "church\n",
      "science\n",
      "supreme court\n",
      "stock\n",
      "congress\n",
      "white house\n",
      "constitution\n",
      "federal\n",
      "syria\n",
      "north korea\n",
      "saudi arabia\n",
      "gop\n",
      "mexico\n",
      "debt\n",
      "fiscal\n",
      "oil\n",
      "medium\n",
      "news\n",
      "racist\n",
      "refugee\n",
      "education\n",
      "maga\n",
      "mitch\n",
      "campaign\n",
      "party\n",
      "poll\n"
     ]
    }
   ],
   "source": [
    "for i in terms[0]:\n",
    "    print(prep.spacy_lemmatizer(text = i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
