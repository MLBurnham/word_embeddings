{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for building the text pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recode PrepDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fa407fa7438>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.load('en', disable = ['tagger', 'parser', 'ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "class PrepDocs:\n",
    "    \"\"\"\n",
    "    Class for either tokenizing or lemmatizing a corpus of documents\n",
    "\n",
    "    self.clean_text will do a basic cleaning that returns lower case, removes\n",
    "    stop words, and removes punctuation\n",
    "\n",
    "    slef.lemmatize_text will do the basic cleaning plus lemmatization\n",
    "\n",
    "    input_dir: directory where text documents to be cleaned are stored\n",
    "    output_dir: directory where cleans documents are to be saved\n",
    "    \"\"\"\n",
    "    def __init__(self, model = 'en', disabled = ['tagger', 'parser', 'ner', 'textcat']):\n",
    "        self.nlp = spacy.load(model, disable = disabled)\n",
    "        self.stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "        self.stopwords.remove()\n",
    "        self.not_allowed = string.punctuation + string.digits\n",
    "\n",
    "    def spacy_tokenizer(self, text):\n",
    "        \"\"\"\n",
    "        Called by the clean_file method\n",
    "        Tokenizes, returns lower case, removes stop words and punctuation,\n",
    "        \"\"\"\n",
    "        # drop subsection numbers\n",
    "        text = re.sub(r\"\\d\\.\", \"\", text)\n",
    "        # tokenize\n",
    "        text = self.nlp(text)\n",
    "        # take the lemma unless it is a pronoun\n",
    "        text = [tok.text.lower().strip() for tok in text]\n",
    "        # drop digits\n",
    "        text = [token for token in text if not token.isdigit()]\n",
    "        # drop stopwords and punctuation\n",
    "        text = [tok for tok in text if (tok not in self.stopwords and tok not in self.not_allowed)]\n",
    "        # rejoin the tokens to a single string\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "\n",
    "    def clean_file(self, doc):\n",
    "        \"\"\"\n",
    "        Used for cleaning a .txt file\n",
    "        \"\"\"\n",
    "        text = open(doc, errors = \"ignore\").read()\n",
    "        text = self.spacy_tokenizer(text)\n",
    "        text_file = open(self.output_dir + \"/\" + doc[0:-4] + '_clean' '.txt',\n",
    "                             'w', errors='replace')\n",
    "        text_file.write(text)\n",
    "\n",
    "    def spacy_lemmatizer(self, text):\n",
    "        \"\"\"\n",
    "        Called by the lemmatize_file method\n",
    "        Tokenizes, lemmatizes, returns lower case, removes stop words and punctuation\n",
    "        \"\"\"\n",
    "        # reload spacy with pat of speech tagger\n",
    "        nlp = spacy.load('en', disable = ['parser', 'ner', 'textcat'])\n",
    "        # drop subsection numbers\n",
    "        text = re.sub(r\"\\d\\.\", \"\", text)\n",
    "        # tokenize\n",
    "        text = nlp(text)\n",
    "        # drop stopwords\n",
    "        # take the lemma unless it is a stopword\n",
    "        text = [tok.lemma_.lower().strip() for tok in text if (tok.text not in self.stopwords and tok.text not in self.not_allowed)]\n",
    "        # drop digits\n",
    "        text = [token for token in text if not token.isdigit()]\n",
    "        # rejoins the tokens to a single string\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "\n",
    "    def lemmatize_file(self, doc):\n",
    "        \"\"\"\n",
    "        Used for lemmatizing a .txt file\n",
    "        \"\"\"\n",
    "        text = open(self.input_dir + \"/\" + doc, errors = \"ignore\").read()\n",
    "        text = self.spacy_lemmatizer(text)\n",
    "        text_file = open(self.output_dir + \"/\" + doc[0:-4] + '_clean' '.txt',\n",
    "                             'w', errors='replace')\n",
    "        text_file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need scotus'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = PrepDocs('none', 'none')\n",
    "prep.spacy_lemmatizer(text = 'we need more scotus\\'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following terms need to be converted via regular expressions:\n",
    "synonyms should be converted to a single term, words that won't be located in both the congressional and troll data set should be eliminated\n",
    "In order to catch compound words using my key tokens such as \"moscowmitch\" replace each \"token\" with \" token \" via regular expression\n",
    "\n",
    "\n",
    "impeachment > impeach\n",
    "\n",
    "bernie sanders > sanders\n",
    "\n",
    "bernie > sanders\n",
    "\n",
    "abort > abortion\n",
    "\n",
    "global warming > climate change\n",
    "\n",
    "climate change > climatechange\n",
    "\n",
    "electoral college > electoralcollege\n",
    "\n",
    "pence > mikepence\n",
    "\n",
    "mike pence > mikepence\n",
    "\n",
    "citizens united > citizensunited\n",
    "\n",
    "lgbt > lgbtq\n",
    "\n",
    "gay > lgbtq\n",
    "\n",
    "lesbian > lgbtq\n",
    "\n",
    "homosexual > lgbtq\n",
    "\n",
    "supreme court > scotus\n",
    "\n",
    "witch hunt > witchhunt\n",
    "\n",
    "north korea > northkorea\n",
    "\n",
    "saudi arabia > saudi\n",
    "\n",
    "media > newsmedia\n",
    "\n",
    "corruption > corrupt\n",
    "\n",
    "obamacare > ACA\n",
    "\n",
    "affordable care act > ACA\n",
    "\n",
    "donald trump > trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>constitution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>federal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>syria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>north korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>saudi arabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>gop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>debt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>fiscal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>refugee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>maga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>mitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>campaign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>poll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "80  constitution\n",
       "81       federal\n",
       "82         syria\n",
       "83   north korea\n",
       "84  saudi arabia\n",
       "85           gop\n",
       "86        mexico\n",
       "87          debt\n",
       "88        fiscal\n",
       "89           oil\n",
       "90         media\n",
       "91          news\n",
       "92        racist\n",
       "93       refugee\n",
       "94     education\n",
       "95          maga\n",
       "96         mitch\n",
       "97      campaign\n",
       "98         party\n",
       "99          poll"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = pd.read_csv('Political terms.csv', header = None)\n",
    "terms.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump\n",
      "president\n",
      "healthcare\n",
      "border\n",
      "wall\n",
      "democrat\n",
      "republican\n",
      "liberal\n",
      "conservative\n",
      "abortion abort\n",
      "clinton\n",
      "sander\n",
      "socialist\n",
      "economy\n",
      "job\n",
      "impeachment\n",
      "potus\n",
      "obama\n",
      "russia\n",
      "mueller\n",
      "collusion\n",
      "military\n",
      "budget\n",
      "market\n",
      "trade\n",
      "vote\n",
      "democracy\n",
      "gun\n",
      "firearm\n",
      "assualt rifle\n",
      "agriculture\n",
      "woman\n",
      "business\n",
      "tax\n",
      "medicare\n",
      "police\n",
      "immigration\n",
      "insurance\n",
      "climate\n",
      "corrupt\n",
      "progressive\n",
      "electoral college\n",
      "judge\n",
      "court\n",
      "gerrymander\n",
      "pelosi\n",
      "mcconnell\n",
      "penny\n",
      "citizen unite\n",
      "dreamer\n",
      "lgbtq\n",
      "obamacare\n",
      "scotus\n",
      "partisan\n",
      "patriot\n",
      "welfare\n",
      "privilege\n",
      "minority\n",
      "muslim\n",
      "god\n",
      "religion\n",
      "administration\n",
      "politic\n",
      "\n",
      "fair\n",
      "witch hunt\n",
      "warren\n",
      "biden\n",
      "security\n",
      "terrorism\n",
      "pentagon\n",
      "senate\n",
      "rich\n",
      "american\n",
      "church\n",
      "science\n",
      "supreme court\n",
      "stock\n",
      "congress\n",
      "white house\n",
      "constitution\n",
      "federal\n",
      "syria\n",
      "north korea\n",
      "saudi arabia\n",
      "gop\n",
      "mexico\n",
      "debt\n",
      "fiscal\n",
      "oil\n",
      "medium\n",
      "news\n",
      "racist\n",
      "refugee\n",
      "education\n",
      "maga\n",
      "mitch\n",
      "campaign\n",
      "party\n",
      "poll\n"
     ]
    }
   ],
   "source": [
    "for i in terms[0]:\n",
    "    print(prep.spacy_lemmatizer(text = i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
