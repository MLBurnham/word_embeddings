{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for building the text pre-processing pipeline\n",
    "\n",
    "Rough outline of the pipeline:\n",
    "1. remove punctuation, numbers, capitalization from all text. Return full string rather than individual tokens\n",
    "2. isolate the key word of interest in case it appears in any hashtags\n",
    "3. replace alternatives/synonyms with a single token for that word\n",
    "4. label that token based on party affiliation\n",
    "5. remove stopwords, tokenize, lematize text\n",
    "6. train word2vec model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import demoji\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recode PrepDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fa407fa7438>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.load('en', disable = ['tagger', 'parser', 'ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "class PrepDocs:\n",
    "    \"\"\"\n",
    "    Class for either tokenizing or lemmatizing a corpus of documents\n",
    "    \n",
    "    key_tokens is a list of tokens your interested in locating in the text\n",
    "    \n",
    "    key_synonyms is meant to be a dictionary of synonyms and their associated key tokens.\n",
    "    This is helpful if used in conjunction with the replace_keyword method so that all\n",
    "    synonyms can be replaced by a single token.\n",
    "\n",
    "    self.clean_text will do a basic cleaning that returns lower case, removes\n",
    "    stop words, and removes punctuation\n",
    "\n",
    "    slef.lemmatize_text will do the basic cleaning plus lemmatization\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model = 'en', disabled = ['tagger', 'parser', 'ner', 'textcat'], key_tokens = None, key_synonyms = None):\n",
    "        self.nlp = spacy.load(model, disable = disabled)\n",
    "        self.stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "        self.stopwords.remove()\n",
    "        self.not_allowed = string.punctuation + string.digits\n",
    "        self.key_tokens = key_tokens\n",
    "        self.key_synonyms = key_words\n",
    "\n",
    "    def spacy_tokenizer(self, text):\n",
    "        \"\"\"\n",
    "        Called by the clean_file method\n",
    "        Tokenizes, returns lower case, removes stop words and punctuation,\n",
    "        \"\"\"\n",
    "        # drop subsection numbers\n",
    "        text = re.sub(r\"\\d\\.\", \"\", text)\n",
    "        # tokenize\n",
    "        text = self.nlp(text)\n",
    "        # take the lemma unless it is a pronoun\n",
    "        text = [tok.text.lower().strip() for tok in text]\n",
    "        # drop digits\n",
    "        text = [token for token in text if not token.isdigit()]\n",
    "        # drop stopwords and punctuation\n",
    "        text = [tok for tok in text if (tok not in self.stopwords and tok not in self.not_allowed)]\n",
    "        # rejoin the tokens to a single string\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "\n",
    "    def spacy_lemmatizer(self, text):\n",
    "        \"\"\"\n",
    "        Called by the lemmatize_file method\n",
    "        Tokenizes, lemmatizes, returns lower case, removes stop words and punctuation\n",
    "        \"\"\"\n",
    "        # reload spacy with pat of speech tagger\n",
    "        nlp = spacy.load('en', disable = ['parser', 'ner', 'textcat'])\n",
    "        # tokenize\n",
    "        text = nlp(text)\n",
    "        # drop stopwords\n",
    "        # take the lemma unless it is a stopword\n",
    "        text = [tok.lemma_.lower().strip() for tok in text if (tok.text not in self.stopwords and tok.text not in self.not_allowed)]\n",
    "        # drop digits\n",
    "        text = [token for token in text if not token.isdigit()]\n",
    "        # rejoins the tokens to a single string\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "    \n",
    "    def twitter_preprocess(self, text):\n",
    "        \"\"\"\n",
    "        This functions does some preliminary cleaning for twitter text. it will:\n",
    "        1. Remove all pound symbols from text\n",
    "        2. If key_tokens is defined, isolate key words from hashtags so they can be treated as individual tokens\n",
    "        3. Remove emoji\n",
    "        4. Remove websites from the text\n",
    "        \"\"\"\n",
    "        # remove pound signs\n",
    "        text = re.sub('#', '', text)\n",
    "        # isolate key words located in hashtags\n",
    "        if self.key_tokens is not None:\n",
    "            for token in self.key_tokens\n",
    "                text = re.sub(token, ' ' + token + ' ', text)\n",
    "                text = re.sub('\\s+', ' ', text)\n",
    "        # remove emoji\n",
    "        text = demoji.replace(text, '')\n",
    "        # remove websites\n",
    "        text = re.sub('https:\\S*', '', text)\n",
    "        # remove excess white space\n",
    "        text = re.sub('\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def replace_keyword(token, replacement, text):\n",
    "        \"\"\"\n",
    "        Finds a specific token and replaces it within a given text.\n",
    "        Generally this is used to replace synonyms for your key words of interest.\n",
    "        Use in conjunction with the key words dicitonary\n",
    "        \"\"\"\n",
    "        text = re.sub(token, replacement, text)\n",
    "        text = re.sub('\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def tag_tokens(self, text_list, tag_list, token):\n",
    "        \"\"\"\n",
    "        Finds all instances of a user defined token within a specific corpus and tags it to\n",
    "        a specific group. Pass a list of text documents, and a list of associated tags for those documents\n",
    "        such as two columns from a data frame. \n",
    "        \"\"\"\n",
    "        for i in range(len(text_list)):\n",
    "            text_list[i] = re.sub(token, token + \"_\" + tag_list[i], text_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(self, text):\n",
    "    text = re.sub('#', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert spaces between key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_keyword(self, word, text):\n",
    "    text = re.sub(word, ' ' + word + ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key word dicitonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# a list of all key tokens\n",
    "key_tokens = ['trump',\n",
    "'president',\n",
    "'healthcare',\n",
    "'border',\n",
    "'wall',\n",
    "'democrat',\n",
    "'republican',\n",
    "'liberal',\n",
    "'conservative',\n",
    "'abortion',\n",
    "'clinton',\n",
    "'sanders',\n",
    "'socialist',\n",
    "'economy',\n",
    "'jobs',\n",
    "'impeach',\n",
    "'obama',\n",
    "'russia',\n",
    "'mueller',\n",
    "'collusion',\n",
    "'military',\n",
    "'budget',\n",
    "'market',\n",
    "'trade',\n",
    "'vote',\n",
    "'democracy',\n",
    "'gun',\n",
    "'agriculture',\n",
    "'women',\n",
    "'business',\n",
    "'tax',\n",
    "'medicare',\n",
    "'police',\n",
    "'immigration',\n",
    "'insurance',\n",
    "'climatechange',\n",
    "'corrupt',\n",
    "'electoralcollege',\n",
    "'judge',\n",
    "'court',\n",
    "'gerrymander',\n",
    "'pelosi',\n",
    "'mcconnell',\n",
    "'mikepence',\n",
    "'citizensunited',\n",
    "'daca',\n",
    "'dreamers',\n",
    "'lgbtq',\n",
    "'ACA',\n",
    "'scotus',\n",
    "'partisan',\n",
    "'patriot',\n",
    "'welfare',\n",
    "'privilege',\n",
    "'minority',\n",
    "'islam',\n",
    "'muslim',\n",
    "'christian',\n",
    "'god',\n",
    "'religion',\n",
    "'administration',\n",
    "'politics',\n",
    "'fair',\n",
    "'witchhunt',\n",
    "'warren',\n",
    "'biden',\n",
    "'security',\n",
    "'terrorism',\n",
    "'defense',\n",
    "'pentagon',\n",
    "'homelandsecurity',\n",
    "'senate',\n",
    "'wealth',\n",
    "'american',\n",
    "'church',\n",
    "'science',\n",
    "'stockmarket',\n",
    "'congress',\n",
    "'whitehouse',\n",
    "'constitution',\n",
    "'federal',\n",
    "'syria',\n",
    "'northkorea',\n",
    "'saudiarabia',\n",
    "'mexico',\n",
    "'debt',\n",
    "'fiscal',\n",
    "'oil',\n",
    "'media',\n",
    "'cnn',\n",
    "'fox',\n",
    "'news',\n",
    "'racist',\n",
    "'refugee',\n",
    "'education',\n",
    "'maga',\n",
    "'campaign',\n",
    "'party',\n",
    "'poll']\n",
    "\n",
    "f = open(\"Meta Data/key_tokens.pkl\",\"wb\")\n",
    "pickle.dump(key_tokens,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald trump\n",
      "president trump\n"
     ]
    }
   ],
   "source": [
    "# returns the synonyms for each key token\n",
    "for key, token in key_synonyms.items():\n",
    "    if token == key_tokens[0]:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# this dicitonary can be used to replace key word synonyms with a single token\n",
    "# get a list of the keys relevant, pass to the replace_keyword function\n",
    "\n",
    "key_synonyms = {\n",
    "    'donald trump': 'trump',\n",
    "    'president trump': 'trump',\n",
    "    'health care': 'healthcare',\n",
    "    'dems': 'democrat',\n",
    "    'gop': 'republican',\n",
    "    'left wing': 'liberal',\n",
    "    'leftist': 'liberal',\n",
    "    'progressive': 'liberal',\n",
    "    'right wing': 'conservative',\n",
    "    'abort': 'abortion',\n",
    "    'bernie sanders': 'sanders',\n",
    "    'bernie': 'sanders',\n",
    "    ' econ ': ' economy ',\n",
    "    'economics': 'economy',\n",
    "    'impeachment': 'impeach',\n",
    "    'barack obama': 'obama',\n",
    "    'robert mueller': 'mueller',\n",
    "    'armed forces': 'military',\n",
    "    'armed services': 'military',\n",
    "    'firearm': 'gun',\n",
    "    'assault rifle': 'gun',\n",
    "    'climate change': 'climatechange',\n",
    "    'global warming': 'climatechange',\n",
    "    'corruption': 'corrupt',\n",
    "    'corrupted': 'corrupt',\n",
    "    'electoral college': 'electoralcollege',\n",
    "    'court justice': 'judge',\n",
    "    'nancy pelosi': 'pelosi',\n",
    "    'mitch mcconnell': 'mcconnell',\n",
    "    'vice president mike pence': 'mikepence',\n",
    "    'vice president pence' : 'mikepence',\n",
    "    'mike pence': 'mikepence',\n",
    "    ' pence ': ' mikepence ',\n",
    "    'citizens united': 'citizensunited',\n",
    "    'deferred action for childhood arrivals': 'daca',\n",
    "    'lgbt': 'lgbtq',\n",
    "    'affordable care act': 'aca',\n",
    "    'supreme court of the united states': 'scotus',\n",
    "    'us supreme court': 'scotus',\n",
    "    'united states supreme court': 'scotus',\n",
    "    'supreme court': 'scotus',\n",
    "    'islamic': 'islam',\n",
    "    'christianity': ' christian',\n",
    "    'political': 'politics',\n",
    "    'witch hunt': 'witchhunt',\n",
    "    'elizabeth warren': 'warren',\n",
    "    'counterterrorism': 'terrorism',\n",
    "    'counter-terrorism': 'terrorism',\n",
    "    'homeland security': 'homelandsecurity',\n",
    "    'us senate': 'senate',\n",
    "    'united states senate': 'senate',\n",
    "    'senatorial': 'senate',\n",
    "    'rich': 'wealthy',\n",
    "    'wealthy': ' wealth',\n",
    "    'billionaire': 'wealthy',\n",
    "    'temple': 'church',\n",
    "    'stock market': 'stockmarket',\n",
    "    'stocks': 'stockmarket',\n",
    "    'congressional': 'congress',\n",
    "    'white house': 'whitehouse',\n",
    "    'north korean': 'northkorea',\n",
    "    'north korea': 'northkorea',\n",
    "    'saudi arabian': 'saudiarabia',\n",
    "    'saudi arabia': 'saudiarabia',\n",
    "    ' saudi ': ' saudiarabia ',\n",
    "    'mexican': 'mexico',\n",
    "    'fox news': 'fox',\n",
    "    'cable news network': 'cnn',\n",
    "    'make america great again': 'maga'\n",
    "}\n",
    "\n",
    "f = open(\"Meta Data/synonym_dictionary.pkl\",\"wb\")\n",
    "pickle.dump(key_words,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_keyword(token, replacement, text):\n",
    "    text = re.sub(token, replacement, text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'president donald trump is an american hero'\n",
    "for key in list(key_words)[0:2]:\n",
    "    text = replace_keywords(key, key_words[key], text)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i','a', 'about', 'am', 'an', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'how', 'in', 'is', 'it', 'of', 'on', 'or', 'that', \n",
    "             'the', 'this', 'to', 'was', 'what', 'when', 'where', 'who', 'will', 'with', 'the']\n",
    "\n",
    "f = open(\"Meta Data/stopwords.pkl\",\"wb\")\n",
    "pickle.dump(stopwords,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‘re',\n",
       " 'formerly',\n",
       " 'everything',\n",
       " 'while',\n",
       " 'eleven',\n",
       " 'enough',\n",
       " \"'ll\",\n",
       " 'am',\n",
       " 'seeming',\n",
       " \"n't\",\n",
       " 'hence',\n",
       " 'will',\n",
       " 'did',\n",
       " 'ever',\n",
       " 'make',\n",
       " 'with',\n",
       " 'i',\n",
       " 'whoever',\n",
       " 'what',\n",
       " 'whenever',\n",
       " 'noone',\n",
       " 'where',\n",
       " 'anyway',\n",
       " 'after',\n",
       " 'unless',\n",
       " 'n‘t',\n",
       " 'everyone',\n",
       " 'done',\n",
       " 'then',\n",
       " 'each',\n",
       " 'whereafter',\n",
       " 'show',\n",
       " 'any',\n",
       " 'so',\n",
       " 'she',\n",
       " 'that',\n",
       " 'out',\n",
       " 'always',\n",
       " 'meanwhile',\n",
       " 'next',\n",
       " 'an',\n",
       " 'elsewhere',\n",
       " 'nobody',\n",
       " 'few',\n",
       " 'could',\n",
       " 'therein',\n",
       " 'does',\n",
       " 'front',\n",
       " 'himself',\n",
       " 'were',\n",
       " 'thereby',\n",
       " 'such',\n",
       " 'seems',\n",
       " 'or',\n",
       " 'yours',\n",
       " 'beside',\n",
       " 'another',\n",
       " 'quite',\n",
       " 'and',\n",
       " 'name',\n",
       " 'those',\n",
       " 'do',\n",
       " 'however',\n",
       " 'except',\n",
       " 'until',\n",
       " '’m',\n",
       " 'sixty',\n",
       " 'ca',\n",
       " 'of',\n",
       " '‘s',\n",
       " 'indeed',\n",
       " 'together',\n",
       " 'many',\n",
       " 'moreover',\n",
       " 'they',\n",
       " 'around',\n",
       " \"'re\",\n",
       " '’s',\n",
       " 'too',\n",
       " 'under',\n",
       " 'via',\n",
       " 'toward',\n",
       " 'anyhow',\n",
       " 'therefore',\n",
       " 'hereupon',\n",
       " 'been',\n",
       " '‘m',\n",
       " 'amongst',\n",
       " 'should',\n",
       " 'side',\n",
       " 'hers',\n",
       " 'hereby',\n",
       " 'several',\n",
       " 'when',\n",
       " 'anyone',\n",
       " 'namely',\n",
       " 'one',\n",
       " 'your',\n",
       " 'please',\n",
       " 'nothing',\n",
       " 'otherwise',\n",
       " 'here',\n",
       " 'sometimes',\n",
       " 'within',\n",
       " 'own',\n",
       " 'cannot',\n",
       " 'wherever',\n",
       " 'there',\n",
       " 'forty',\n",
       " 'over',\n",
       " 'former',\n",
       " 'used',\n",
       " 'anywhere',\n",
       " 'him',\n",
       " 'both',\n",
       " 'using',\n",
       " 'due',\n",
       " 'give',\n",
       " 'doing',\n",
       " 'all',\n",
       " 'often',\n",
       " 'you',\n",
       " 'something',\n",
       " 'less',\n",
       " 'eight',\n",
       " \"'m\",\n",
       " 'just',\n",
       " 'has',\n",
       " 'through',\n",
       " 'into',\n",
       " 'us',\n",
       " 'about',\n",
       " 'amount',\n",
       " '’ll',\n",
       " 'others',\n",
       " 'along',\n",
       " 'throughout',\n",
       " 'regarding',\n",
       " 'although',\n",
       " 'call',\n",
       " 'being',\n",
       " 'else',\n",
       " '‘ll',\n",
       " 'rather',\n",
       " 'itself',\n",
       " 'per',\n",
       " 'between',\n",
       " 'these',\n",
       " 'herein',\n",
       " 'whole',\n",
       " 'still',\n",
       " 'as',\n",
       " 'well',\n",
       " 'whatever',\n",
       " 'be',\n",
       " 'whereas',\n",
       " 'can',\n",
       " 'above',\n",
       " 'seem',\n",
       " 'becoming',\n",
       " 'ten',\n",
       " 'but',\n",
       " 'become',\n",
       " 'at',\n",
       " 'almost',\n",
       " 'three',\n",
       " 're',\n",
       " 'below',\n",
       " 'whether',\n",
       " 'how',\n",
       " 'third',\n",
       " 'ourselves',\n",
       " 'other',\n",
       " 'them',\n",
       " 'also',\n",
       " 'their',\n",
       " 'seemed',\n",
       " 'me',\n",
       " 'same',\n",
       " 'yourself',\n",
       " 'may',\n",
       " 'against',\n",
       " 'last',\n",
       " 'thereupon',\n",
       " 'empty',\n",
       " 'why',\n",
       " 'thus',\n",
       " 'alone',\n",
       " 'hereafter',\n",
       " 'was',\n",
       " 'bottom',\n",
       " 'hundred',\n",
       " 'the',\n",
       " 'afterwards',\n",
       " 'now',\n",
       " 'themselves',\n",
       " 'its',\n",
       " 'nowhere',\n",
       " '‘d',\n",
       " 'some',\n",
       " 'fifteen',\n",
       " 'least',\n",
       " 'everywhere',\n",
       " 'say',\n",
       " 'see',\n",
       " 'among',\n",
       " 'again',\n",
       " 'whence',\n",
       " 'yet',\n",
       " 'ours',\n",
       " 'every',\n",
       " 'not',\n",
       " 'on',\n",
       " '’re',\n",
       " 'by',\n",
       " '’ve',\n",
       " 'a',\n",
       " 'we',\n",
       " 'thru',\n",
       " 'four',\n",
       " 'full',\n",
       " 'nor',\n",
       " 'myself',\n",
       " 'somehow',\n",
       " 'in',\n",
       " \"'d\",\n",
       " '’d',\n",
       " 'whither',\n",
       " 'without',\n",
       " 'herself',\n",
       " 'his',\n",
       " 'various',\n",
       " 'it',\n",
       " 'either',\n",
       " 'neither',\n",
       " 'had',\n",
       " 'sometime',\n",
       " 'more',\n",
       " 'mostly',\n",
       " 'back',\n",
       " 'off',\n",
       " 'across',\n",
       " 'mine',\n",
       " 'to',\n",
       " 'my',\n",
       " 'became',\n",
       " 'her',\n",
       " 'beyond',\n",
       " 'take',\n",
       " 'which',\n",
       " 'upon',\n",
       " 'top',\n",
       " 'yourselves',\n",
       " 'go',\n",
       " 'latter',\n",
       " 'even',\n",
       " 'might',\n",
       " 'who',\n",
       " 'once',\n",
       " 'must',\n",
       " 'nine',\n",
       " 'five',\n",
       " 'anything',\n",
       " 'are',\n",
       " 'really',\n",
       " 'six',\n",
       " 'because',\n",
       " 'someone',\n",
       " 'is',\n",
       " 'get',\n",
       " 'somewhere',\n",
       " 'two',\n",
       " 'becomes',\n",
       " 'whereupon',\n",
       " 'very',\n",
       " 'keep',\n",
       " 'never',\n",
       " 'move',\n",
       " 'serious',\n",
       " 'thence',\n",
       " 'up',\n",
       " 'no',\n",
       " 'down',\n",
       " 'twenty',\n",
       " 'though',\n",
       " 'he',\n",
       " 'for',\n",
       " 'perhaps',\n",
       " 'if',\n",
       " 'whose',\n",
       " 'nevertheless',\n",
       " 'since',\n",
       " 'behind',\n",
       " 'from',\n",
       " 'only',\n",
       " 'much',\n",
       " 'latterly',\n",
       " 'twelve',\n",
       " '‘ve',\n",
       " \"'s\",\n",
       " 'none',\n",
       " 'wherein',\n",
       " 'n’t',\n",
       " \"'ve\",\n",
       " 'further',\n",
       " 'before',\n",
       " 'would',\n",
       " 'than',\n",
       " 'first',\n",
       " 'during',\n",
       " 'have',\n",
       " 'besides',\n",
       " 'most',\n",
       " 'made',\n",
       " 'this',\n",
       " 'our',\n",
       " 'beforehand',\n",
       " 'onto',\n",
       " 'whom',\n",
       " 'fifty',\n",
       " 'put',\n",
       " 'already',\n",
       " 'towards',\n",
       " 'thereafter',\n",
       " 'whereby']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-pron- website https://t.co/21alosdjf90'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = PrepDocs('none', 'none')\n",
    "prep.stopwords = stopwords\n",
    "prep.not_allowed = string.punctuation + string.digits + 'https:\\S*'\n",
    "prep.spacy_lemmatizer(text = 'this is my website: https://t.co/21alosdjf90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'a',\n",
       " 'about',\n",
       " 'am',\n",
       " 'an',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'by',\n",
       " 'for',\n",
       " 'from',\n",
       " 'how',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'of',\n",
       " 'on',\n",
       " 'or',\n",
       " 'that',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'was',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'who',\n",
       " 'will',\n",
       " 'with',\n",
       " 'the',\n",
       " 'https:\\\\S*']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### The following terms need to be converted via regular expressions:\n",
    "1. synonyms should be converted to a single term, words that won't be located in both the congressional and troll data set should be eliminated In order to catch compound words using my key tokens such as \"moscowmitch\" replace each \"token\" with \" token \" via regular expression\n",
    "\n",
    "2. Remove hastags\n",
    "\n",
    "\n",
    "impeachment > impeach\n",
    "\n",
    "bernie sanders > sanders\n",
    "\n",
    "bernie > sanders\n",
    "\n",
    "abort > abortion\n",
    "\n",
    "global warming > climate change\n",
    "\n",
    "climate change > climatechange\n",
    "\n",
    "electoral college > electoralcollege\n",
    "\n",
    "pence > mikepence\n",
    "\n",
    "mike pence > mikepence\n",
    "\n",
    "citizens united > citizensunited\n",
    "\n",
    "lgbt > lgbtq\n",
    "\n",
    "gay > lgbtq\n",
    "\n",
    "lesbian > lgbtq\n",
    "\n",
    "homosexual > lgbtq\n",
    "\n",
    "supreme court > scotus\n",
    "\n",
    "witch hunt > witchhunt\n",
    "\n",
    "north korea > northkorea\n",
    "\n",
    "saudi arabia > saudi\n",
    "\n",
    "media > newsmedia\n",
    "\n",
    "corruption > corrupt\n",
    "\n",
    "corrupted > corrupt\n",
    "\n",
    "obamacare > ACA\n",
    "\n",
    "affordable care act > ACA\n",
    "\n",
    "donald trump > trump\n",
    "\n",
    "assault rifle > gun\n",
    "\n",
    "firearm > gun\n",
    "\n",
    "potus > president\n",
    "\n",
    "islamic > islam\n",
    "\n",
    "racism > racist\n",
    "\n",
    "mitch mcconnell > mitchmcconnell\n",
    "\n",
    "mcconnell > mitchmcconnell\n",
    "\n",
    "nancy pelosi > nancypelosi\n",
    "\n",
    "pelosi > nancypelosi\n",
    "\n",
    "taxing > tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>constitution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>federal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>syria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>north korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>saudi arabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>gop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>debt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>fiscal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>refugee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>maga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>mitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>campaign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>poll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "80  constitution\n",
       "81       federal\n",
       "82         syria\n",
       "83   north korea\n",
       "84  saudi arabia\n",
       "85           gop\n",
       "86        mexico\n",
       "87          debt\n",
       "88        fiscal\n",
       "89           oil\n",
       "90         media\n",
       "91          news\n",
       "92        racist\n",
       "93       refugee\n",
       "94     education\n",
       "95          maga\n",
       "96         mitch\n",
       "97      campaign\n",
       "98         party\n",
       "99          poll"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = pd.read_csv('Political terms.csv', header = None)\n",
    "terms.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump\n",
      "president\n",
      "healthcare\n",
      "border\n",
      "wall\n",
      "democrat\n",
      "republican\n",
      "liberal\n",
      "conservative\n",
      "abortion abort\n",
      "clinton\n",
      "sander\n",
      "socialist\n",
      "economy\n",
      "job\n",
      "impeachment\n",
      "potus\n",
      "obama\n",
      "russia\n",
      "mueller\n",
      "collusion\n",
      "military\n",
      "budget\n",
      "market\n",
      "trade\n",
      "vote\n",
      "democracy\n",
      "gun\n",
      "firearm\n",
      "assualt rifle\n",
      "agriculture\n",
      "woman\n",
      "business\n",
      "tax\n",
      "medicare\n",
      "police\n",
      "immigration\n",
      "insurance\n",
      "climate\n",
      "corrupt\n",
      "progressive\n",
      "electoral college\n",
      "judge\n",
      "court\n",
      "gerrymander\n",
      "pelosi\n",
      "mcconnell\n",
      "penny\n",
      "citizen unite\n",
      "dreamer\n",
      "lgbtq\n",
      "obamacare\n",
      "scotus\n",
      "partisan\n",
      "patriot\n",
      "welfare\n",
      "privilege\n",
      "minority\n",
      "muslim\n",
      "god\n",
      "religion\n",
      "administration\n",
      "politic\n",
      "\n",
      "fair\n",
      "witch hunt\n",
      "warren\n",
      "biden\n",
      "security\n",
      "terrorism\n",
      "pentagon\n",
      "senate\n",
      "rich\n",
      "american\n",
      "church\n",
      "science\n",
      "supreme court\n",
      "stock\n",
      "congress\n",
      "white house\n",
      "constitution\n",
      "federal\n",
      "syria\n",
      "north korea\n",
      "saudi arabia\n",
      "gop\n",
      "mexico\n",
      "debt\n",
      "fiscal\n",
      "oil\n",
      "medium\n",
      "news\n",
      "racist\n",
      "refugee\n",
      "education\n",
      "maga\n",
      "mitch\n",
      "campaign\n",
      "party\n",
      "poll\n"
     ]
    }
   ],
   "source": [
    "for i in terms[0]:\n",
    "    print(prep.spacy_lemmatizer(text = i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key token tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"find out about how #donald trump lied at https://t.co/asdfk!lasd08\", 'this is second test sentance trump', \n",
    "        'did you watch the trump impeachment hearings today?']\n",
    "tag_list = ['D', \"R\", \"D\"]\n",
    "\n",
    "def tag_tokens(text_list, token, tag_list):\n",
    "    for i in range(len(text_list)):\n",
    "        text_list[i] = re.sub(token, token + \"_\" + tag_list[i], text_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['find out about how #donald trump_D lied at https://t.co/asdfk!lasd08',\n",
       " 'this is second test sentance trump_R',\n",
       " 'did you watch the trump_D impeachment hearings today?']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_tokens(text, 'trump', tag_list)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_tweets = pd.read_csv('Data/aggregated_tweets.csv')\n",
    "text = aggregated_tweets['text']\n",
    "text[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'At what point did due process become \"cosmetic\" in this country? I don\\'t remember learning that in law school. 🤔 https://t.co/ew1hbvbWBj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At what point did due process become \"cosmetic\" in this country? I don\\'t remember learning that in law school.  https://t.co/ew1hbvbWBj'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demoji.replace(test, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find out about how donald trump lied at \n"
     ]
    }
   ],
   "source": [
    "test_tokens = ['trump', 'token']\n",
    "text = \"find out about how #donaldtrumplied at https://t.co/asdfk!lasd08\"\n",
    "tester = 1\n",
    "def twitter_preprocess(text):\n",
    "    \"\"\"\n",
    "    This functions does some preliminary cleaning for twitter text. it will:\n",
    "    1. Remove all pound symbols from text\n",
    "    2. Isolate key words located within hashtags so they can be treated as individual tokens\n",
    "    3. Remove websites from the text\n",
    "    \"\"\"\n",
    "    # remove pound signs\n",
    "    text = re.sub('#', '', text)\n",
    "    # isolate key words located in hashtags\n",
    "    if tester is not None:\n",
    "        for token in key_tokens:\n",
    "            text = re.sub(token, ' ' + token + ' ', text)\n",
    "            text = re.sub('\\s+', ' ', text)\n",
    "    # remove websites\n",
    "    text = re.sub('https:\\S*', '', text)\n",
    "    print(text)\n",
    "\n",
    "twitter_preprocess(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
