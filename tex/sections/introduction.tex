\documentclass[../embeddings.tex]{subfiles}
 
\begin{document}
This paper examines the use of word embeddings for both the qualitative and quantitative research of ideologies. Word embeddings are language models primarily used to boost the performance of machine learning algorithms on natural language processing (NLP) tasks. They have been wildly successful in this arena because of their ability to capture the many dimensions of semantics in language. For this same reason word embeddings can be used not only as a data source fed to predictive algorithms, but as a standalone model from which social scientists can conduct descriptive and inferential research. 

More specifically, I have three primary objectives: First, I justify this research agenda by briefly reviewing current methods used to leverage text as data, and then examine what exactly word embeddings are and how they alleviate some shortcomings of current techniques. Second, I show that word embeddings can be leveraged to accomplish several of the same tasks dictionary and word count methods are currently used for â€“ namely qualitative or descriptive analysis and scaling or positional estimates of ideological stances. I argue that word embeddings accomplish this in a more robust and theoretically sound manner. Finally, I discuss a research agenda moving forward to explore the most rigorous and practical implementation of word embeddings in social science research.
\end{document}