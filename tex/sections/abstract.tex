\documentclass[../embeddings.tex]{subfiles}
 
\begin{document}
Natural Language Processing has made significant strides on machine learning tasks through the use of word embeddings. These vectorized language models boost predictive capabilities primarily because of how proficient they are at capturing the multidimensional nature of language. I explore how social scientists might be able to leverage word embeddings to capture more accurate information than the widely used “bag-of-words” methods are currently capable of. I provide some background on why embeddings work and how they are used, and then test their efficacy on a common task in political science: ideological position estimates. If find that word embeddings preform well in descriptive tasks pertaining to ideological space, but get mixed results in quantitative position estimates. I conclude that while word embeddings show significant promise,  further work is needed to adapt these tools for social scientists.
\end{document}

