\documentclass[../embeddings.tex]{subfiles}
 
\begin{document}
\subsection{Hypothesis 1: The words in closest proximity to ideologically significant words will highlight points of contention or agreement between groups and can be used for descriptive analysis.}

To test my first hypothesis I examine a list of the ten most similar words as measured by cosine similarity for each word, and a similar list for that word’s counterpart from the other party. I expect that words unique to each list represent ideological divides between the parties, and common words represent points of agreement. As cosine similarity between a word and its counterpart increases, I expect that overlap between the two lists will increase. 

From the disagree set of words I used the word “impeach” because of its relevancy to the news in the time period I collected the data. Because word vectors within the embedding contain 100 dimensions it is impossible to plot them precisely. To compensate, I used principal component analysis (PCA) to reduce the number of dimensions. PCA preforms a linear transformation on the word vectors that combines variables into n principal components that explain the maximum amount of variance. Because each component is an amalgamation of the other dimensions within the word vector, components are not readily interpretable. Rather, this technique is solely used to enable a rough visualization of the data’s separation. Figure X below shows the scatter plot for the word “impeach” and the ten most similar words by party after applying PCA to reduce the dimensionality down to three. With three dimensional PCA I am able to explain roughly 75\% of the variance between word vectors, meaning that while the plot is not an exact visualization of the distance between words, it is a decent approximation.

The plot for “impeach” shows two distinct clouds for the parties with little overlap. As shown in table X, the parties have no common terms among the ten terms most similar to their version of impeach. More significantly for this hypothesis, the most similar terms clearly reflect party positions. Democrat words feature terms such as “abuseofpower” and “exposethetruth” while Republican words include “partisan”, “charade”, and “circus.”

I repeated this process for the word “robocall” due to bipartisan legislation to crack down on robocalls during the period of data collection. Figure X and table X show the opposite of “impeach” with significant overlap between the two versions of robocall. Most telling is that one of the closest words to the Democrat’s version of robocall is the republican’s version, suggesting the words are synonymous to some degree. 
Finally, the neutral word “look” exhibits what appears to be largely noise. The two words are largely synonymous, but the nearest neighbors to each words do not appear to carry significant ideological meaning.

\subsection{Hypothesis 2: If text is divided into two groups and separate word vectors for the same word are calculated between the two groups, that word vector will have a lower cosine similarity with its counterpart if the text is labeled by ideological group rather than labeled randomly.}

The second hypothesis is designed to test if word embeddings capture semantic differences between groups and if cosine similarity can be used to quantify those differences. Generally speaking, my expectation is that all words will have a lower cosine similarity with its counterpart when labeled by party rather than randomly, albeit I do expect the difference between party and randomly labeled data to be greatest between words that represent disagreement and smallest between words that represent agreement. I expect near universal difference because even if a word represents a point of agreement between the parties, I still expect slight variation in the party’s talking points. There may be exceptions to this, for example if the two parties were to coordinate a public relations campaign together.

To test this hypothesis, I conducted a permutation test on individual words. I first label each text according to its associated party, and then calculate the observed cosine similarity between a single word and its counterpart. I then randomly permute the text labels, recalculate cosine similarity, and repeat this process 1,000 times. From here I can determine probability that my observed cosine similarity is drawn from my data set of cosine similarities calculated with randomized labels. A low p-value indicates that text labels do  matter, and that the word embedding is capturing semantic differences between the groups. Because this test takes about 14 hours of computational time to complete, I repeated it for only one word from each of my three categories. For the agree and disagree categories I chose “usmca” and “trump” respectively due to their relevance to the news cycle during the period I collected data. A word from the base word list was chosen at random. In this case I used “place.”

Figures X show a histogram of the results 


\subsection{Hypothesis 3: If separate word vectors are calculated for ideologically distinct groups, the word vectors that represent points of disagreement will have a lower cosine similarity than those that represent points of agreement.}

My final hypothesis seeks to test the generalizability of the results from the second hypothesis. To do so, I calculated the cosine similarity for each word and its ideological counterpart in my list. I then calculate the mean cosine similarity for each of the three word groups. Namely, words that represent disagreement, agreement, and non-ideological words. I use a difference of means test to evaluate differences between the groups. I expect that words in the disagree category will have the lowest cosine similarity, the agree will have the highest, and non-ideological will be in the middle. Because the number of words is relatively small, I use a permutation test and bootstrap confidence interval to supplement my results.

There are two primary factors driving these inconclusive results. First are potential shortcomings in my list of terms. For example, I selected the word “police” under the expectation that it would capture different attitudes on police brutality and the Black Lives Matter movement. Generally speaking however, elected officials seemed reluctant to criticize police on social media. Ongoing police brutality surrounding the Hong Kong protests has been denounced by both parties on social media. “Police” therefore did not capture the intended ideological dimensions. Similarly, the word “healthcare” which was included in the disagree list was probably more noise than signal. While there is certainly disagreement between the parties on healthcare, clear party platforms have not emerged and there is significant disagreement within the parties as well. Thus, I would probably need to draw different ideological boundaries to measure this particular concept. 

The second shortcoming is simply that cosine similarity is a bit of a blunt instrument. In their research on synonym extraction, Leeuwenberg et al. showed that cosine similarity alone is a bad indicator to determine if two words are synonymous. Instead, they propose a technique they call relative cosine similarity which examines the similarity between two words relative to other words in the corpus. As a rule of thumb, if relative cosine similarity is greater than 0.10, the two words are more similar than an arbitrary word pair. Accordingly, I repeat the above analysis using relative cosine similarity and present the results in the table X.

Relative cosine similarity reinforces the above findings and appears to give more robust results than simple cosine similarity. The strongest indicator of this is the significant jump in similarity among the base words. While some deviation is to be expected because Democrats and Republicans do not engage in the same topics of discussion with the same frequency, I expect words that can be found in any context and generally carry no ideological significance to be relatively synonymous.

\end{document}